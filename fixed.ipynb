{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb342b1-8c7a-4914-9f17-d5643fdec5fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3747645088.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from nemo.lightning import AutoResume\n",
    "\n",
    "resume = AutoResume(\n",
    "    resume_if_exists=True,\n",
    "    resume_ignore_no_cã„‡heckpoint=False,\n",
    "    resume_from_directory=\"/datasets/soc-20250703225140/nemo_checkpoints/nemotron_49b_super_custom_finetune/2025-07-14_17-42-12/checkpoints/model_name=0--val_loss=0.03-step=449-consumed_samples=900.0-last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e0bfa-0bfd-4d1b-838f-b30fe49d9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nemo_inference.py\n",
    "\n",
    "import torch.distributed\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "import nemo.lightning as nl\n",
    "import re\n",
    "\n",
    "strategy = nl.MegatronStrategy(\n",
    "    tensor_model_parallel_size=2,\n",
    "    pipeline_model_parallel_size=1,\n",
    "    context_parallel_size=1,\n",
    "    sequence_parallel=False,\n",
    "    setup_optimizers=False,\n",
    "    store_optimizer_states=False,\n",
    ")\n",
    "\n",
    "trainer = nl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=2,\n",
    "    num_nodes=1,\n",
    "    strategy=strategy,\n",
    "    plugins=nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "        params_dtype=torch.bfloat16,\n",
    "        pipeline_dtype=torch.bfloat16,\n",
    "        autocast_enabled=False,\n",
    "        grad_reduce_in_fp32=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "source = {\n",
    "    \"mask\": \"User\",\n",
    "    \"system\": \"\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"User\",\n",
    "            \"value\": \"\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "special_tokens = {\n",
    "                \"system_turn_start\": \"<extra_id_0>\",\n",
    "                \"turn_start\": \"<extra_id_1>\",\n",
    "                \"label_start\": \"<extra_id_2>\",\n",
    "                \"end_of_turn\": \"\\n\",\n",
    "                \"end_of_name\": \"\\n\",\n",
    "            }\n",
    "\n",
    "from nemo.collections.nlp.data.language_modeling.megatron.gpt_sft_chat_dataset import _get_header_conversation_type_mask_role\n",
    "# Apply prompt template to be the same format as training\n",
    "header, conversation, data_type, mask_role = _get_header_conversation_type_mask_role(source, special_tokens)\n",
    "prompts = [conversation]\n",
    "\n",
    "from nemo.collections.llm import api\n",
    "results = api.generate(\n",
    "    path=\"/datasets/soc-20250703225140/nemo_checkpoints/nemotron_49b_super_custom_finetune/2025-07-17_13-54-48/checkpoints/model_name=0--val_loss=0.02-step=2459-consumed_samples=4920.0-last\",\n",
    "    prompts=prompts,\n",
    "    trainer=trainer,\n",
    "    inference_params=CommonInferenceParams(\n",
    "        temperature=1.0,\n",
    "        top_p=0,  # greedy decoding\n",
    "        top_k=1,  # greedy decoding\n",
    "        num_tokens_to_generate=50,\n",
    "    ),\n",
    "    text_only=True,\n",
    ")\n",
    "if torch.distributed.get_rank() == 0:\n",
    "    for i, r in enumerate(results):\n",
    "        print(\"=\" * 50)\n",
    "        print(prompts[i])\n",
    "        print(\"*\" * 50)\n",
    "        if match:\n",
    "            print(match.group(0))\n",
    "        else:\n",
    "            print(r)\n",
    "        print(\"=\" * 50)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00c136fb-63d3-4e2e-b991-710658cd2d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accelerator', 'accumulate_grad_batches', 'barebones', 'benchmark', 'callbacks', 'check_val_every_n_epoch', 'default_root_dir', 'detect_anomaly', 'deterministic', 'devices', 'enable_checkpointing', 'enable_model_summary', 'enable_progress_bar', 'fast_dev_run', 'gradient_clip_algorithm', 'gradient_clip_val', 'inference_mode', 'limit_predict_batches', 'limit_test_batches', 'limit_train_batches', 'limit_val_batches', 'log_every_n_steps', 'logger', 'max_epochs', 'max_steps', 'max_time', 'min_epochs', 'min_steps', 'num_nodes', 'num_sanity_val_steps', 'overfit_batches', 'plugins', 'precision', 'profiler', 'reload_dataloaders_every_n_epochs', 'strategy', 'sync_batchnorm', 'use_distributed_sampler', 'val_check_interval']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from nemo.collections import llm\n",
    "\n",
    "## function\n",
    "# inspect.signature(llm.llama33_nemotron_super_49b)\n",
    "# inspect.signature(llm.LlamaNemotronModel)\n",
    "# inspect.signature(llm.llama33_nemotron_super_49b.finetune_recipe)\n",
    "\n",
    "print(dir(llm.llama33_nemotron_super_49b.finetune_recipe().trainer))\n",
    "print((llm.llama33_nemotron_super_49b.finetune_recipe().trainer.limit_val_batches))\n",
    "# print(dir(llm.llama33_nemotron_super_49b.finetune_recipe().resume.resume_from_directory))\n",
    "# print(dir(llm.llama33_nemotron_super_49b.finetune_recipe().resume)))\n",
    "# print(dir(llm.llama33_nemotron_super_49b.finetune_recipe().log))\n",
    "# print(dir(llm.llama33_nemotron_super_49b))\n",
    "# print(dir(llm.LlamaNemotronModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3deef62f-06c3-4785-a8f9-410632e42ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada63b61-4064-4a46-b041-85284c80e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections import llm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import nemo_run as run\n",
    "from nemo import lightning as nl\n",
    "\n",
    "# checkpoint_path=\"/datasets/soc-20250703225140/nemo_checkpoints/nemotron_49b_super_custom_finetune/2025-07-14_00-47-12/checkpoints/model_name=0--val_loss=0.03-step=499-consumed_samples=1000.0-last/weights/\"\n",
    "\n",
    "# from nemo.collections import llm\n",
    "# llm.import_ckpt(model=llm.LlamaNemotronModel(llm.Llama33NemotronSuper49BConfig()), source=checkpoint_path)\n",
    "\n",
    "# llm.LlamaNemotronModel.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
    "\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "\n",
    "sft_ckpt_path=str(next((d for d in Path(\"/datasets/soc-20250703225140/nemo_checkpoints/nemotron_49b_super_custom_finetune/2025-07-14_00-47-12/checkpoints/\").iterdir() if d.is_dir() and d.name.endswith(\"-last\")), None))\n",
    "print(\"We will load SFT checkpoint from:\", sft_ckpt_path)\n",
    "\n",
    "\n",
    "\n",
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=2,\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=2,\n",
    "        num_nodes=1,\n",
    "        strategy=strategy,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "prompts = [\n",
    "    \"How many r's are in the word 'strawberry'?\",\n",
    "    \"Which number is bigger? 10.119 or 10.19?\",\n",
    "]\n",
    "\n",
    "def configure_inference():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(sft_ckpt_path),\n",
    "        trainer=trainer(),\n",
    "        prompts=prompts,\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=8192, top_p=0.95),\n",
    "        output_path=\"sft_prediction.jsonl\",\n",
    "    )\n",
    "\n",
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 2) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "        \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\",\n",
    "        \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_inference(), executor=local_executor_torchrun())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a043c6ce-5132-4106-985b-6fd29a49bba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He's\n"
     ]
    }
   ],
   "source": [
    "print(\"He's\".replace(\"'\", \"\\\\'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c78c5ae6-4ead-429b-bb72-7f4380a560ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clinical Pharmacogenetics Implementation Consortium (CPIC) Guidelines for Ivacaftor Therapy in the Context of CFTR Genotype (March 2014).pdf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data_process.guideline_names_extraction\n",
    "# get_guideline_name()\n",
    "import os\n",
    "cpic_guideline_name_list = os.listdir(\"./data_process/Guidelines\")\n",
    "cpic_guideline_name_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879d6a94-4343-4ac6-8dbb-5619394dfc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” è½‰æ›å®Œæˆï¼š4391 ç­†ï¼ˆè·³é 0 ç­†ï¼‰\n",
      "  â†’ training.jsonl    3073 è¡Œ  ï¼ˆé©—è­‰é€šéï¼‰\n",
      "  â†’ validation.jsonl  658 è¡Œ  ï¼ˆé©—è­‰é€šéï¼‰\n",
      "  â†’ test.jsonl        660 è¡Œ  ï¼ˆé©—è­‰é€šéï¼‰\n",
      "\n",
      "ğŸ‰  å…¨éƒ¨å®Œæˆï¼å¯ç›´æ¥äº¤çµ¦ NeMo é€²è¡Œ fine-tuneã€‚\n"
     ]
    }
   ],
   "source": [
    "## æœ€é‡è¦ç¨‹å¼ç¢¼\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "convert_nemo_dataset.py\n",
    "\n",
    "å°‡ dataset.json è½‰æˆ NeMo Chat-SFT æ‰€éœ€çš„\n",
    "training.jsonl / validation.jsonl / test.jsonlï¼ˆä¸‰æª”ï¼‰ã€‚\n",
    "\n",
    "ç‰¹é»\n",
    "1. å¾¹åº•æ¸…ç†æ‰€æœ‰éš±è—æ›è¡Œ ( \\n, \\r, U+2028, U+2029 )\n",
    "2. å° dict / list value å…ˆåš json.dumpsï¼Œä¿è­‰å–®è¡Œ\n",
    "3. æ¯ç­†å¯«å…¥å‰ä»¥ json.dumps() é©—è­‰ï¼›å«æ›è¡Œå°±å¼·åˆ¶ä¸Ÿæ£„\n",
    "4. å¯«æª”å®Œæˆå¾Œå†æ¬¡æ·±åº¦æƒæï¼Œè‹¥ä»æœ‰æ®˜ç•™æ›è¡Œå³æ‹‹éŒ¯\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1.  åŸºæœ¬è¨­å®š\n",
    "# --------------------------------------------------------------------\n",
    "SRC_JSON         = \"/datasets/soc-20250703225140/dataset/dataset.json\"\n",
    "OUT_DIR          = \"/datasets/soc-20250703225140/dataset_split\"\n",
    "SPLIT_RATIOS     = (0.70, 0.15, 0.15)  # train / val / test\n",
    "RANDOM_SEED      = 42\n",
    "NEWLINES         = (\"\\n\", \"\\r\", \"\\u2028\", \"\\u2029\")       # æ‰€æœ‰å¯èƒ½æ®˜ç•™çš„æ›è¡Œç¬¦\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2.  å·¥å…·å‡½å¼\n",
    "# --------------------------------------------------------------------\n",
    "def sanitize(text: Any) -> str:\n",
    "    \"\"\"å°‡ä»»æ„ç‰©ä»¶è½‰ç‚ºã€å–®è¡Œã€å®‰å…¨å­—ä¸²ã€\"\"\"\n",
    "    s = str(text)\n",
    "    # æ›è¡Œç¬¦ â†’ ç©ºç™½\n",
    "    for ch in NEWLINES:\n",
    "        s = s.replace(ch, \" \")\n",
    "    # å…¶ä»–ç©ºç™½å£“ç¸®\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # è·³è„«é›™å¼•è™Ÿ\n",
    "    return s.replace('\"', r'\\\"')\n",
    "\n",
    "\n",
    "def format_answer(ans: Any) -> str:\n",
    "    \"\"\"\n",
    "    æŠŠ answer è½‰æˆ <QUERYCALL>[ ... ]</QUERYCALL>\n",
    "      - è‹¥ç‚º dictï¼šé€ key/value è™•ç†\n",
    "      - å…¶ä»–å‹åˆ¥ï¼šè¦–ç‚ºæ‹’ç­”æ–‡å­—ï¼Œç›´æ¥ sanitize\n",
    "    \"\"\"\n",
    "    if not isinstance(ans, dict):\n",
    "        return sanitize(ans)\n",
    "\n",
    "    parts = []\n",
    "    for k, v in ans.items():\n",
    "        if isinstance(v, (dict, list)):\n",
    "            v = json.dumps(v, separators=(\",\", \":\"))  # å·¢ç‹€è³‡æ–™ â†’ å£“æˆå–®è¡Œ JSON\n",
    "        parts.append(f'{sanitize(k)}=\"{sanitize(v)}\"')\n",
    "    return f\"<QUERYCALL>[{', '.join(parts)}]</QUERYCALL>\"\n",
    "\n",
    "\n",
    "def has_hidden_newline(s: str) -> bool:\n",
    "    \"\"\"åˆ¤æ–·å­—ä¸²æ˜¯å¦ä»å« NEWLINES ä¸­çš„ä»»ä¸€å­—å…ƒ\"\"\"\n",
    "    return any(ch in s for ch in NEWLINES)\n",
    "\n",
    "\n",
    "def deep_scan_jsonl(path: Path) -> List[int]:\n",
    "    \"\"\"\n",
    "    æƒæ .jsonlï¼Œå›å‚³ã€ä»å«éš±è—æ›è¡Œç¬¦ã€çš„è¡Œè™Ÿæ¸…å–®\n",
    "    å‚™è¨»ï¼šä½¿ç”¨ newline=\"\" è®€æª”ï¼Œæ‰ä¸æœƒè¢« Python è‡ªå‹•è½‰è­¯\n",
    "    \"\"\"\n",
    "    bad_lines = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        for ln, raw in enumerate(f, 1):\n",
    "            # å»é™¤çœŸæ­£çš„è¡Œå°¾\n",
    "            if raw.endswith(\"\\r\\n\"):\n",
    "                core = raw[:-2]\n",
    "            elif raw.endswith(\"\\n\"):\n",
    "                core = raw[:-1]\n",
    "            else:\n",
    "                core = raw\n",
    "            if has_hidden_newline(core):\n",
    "                bad_lines.append(ln)\n",
    "    return bad_lines\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3.  ä¸»è¦æµç¨‹\n",
    "# --------------------------------------------------------------------\n",
    "def convert_dataset(\n",
    "    src_json: str,\n",
    "    out_dir: str,\n",
    "    split: tuple = SPLIT_RATIOS,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> None:\n",
    "    random.seed(seed)\n",
    "\n",
    "    # è®€å…¥åŸå§‹ JSON\n",
    "    with open(src_json, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"è¼¸å…¥æª”æ¡ˆå¿…é ˆæ˜¯ JSON listã€‚\")\n",
    "\n",
    "    processed: List[Dict] = []\n",
    "    skipped = 0\n",
    "\n",
    "    for idx, item in enumerate(data, 1):\n",
    "        if not (isinstance(item, dict) and \"question\" in item and \"answer\" in item):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        record = {\n",
    "            \"system\": \"\",\n",
    "            \"mask\": \"User\",\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"User\",      \"value\": sanitize(item[\"question\"])},\n",
    "                {\"from\": \"Assistant\", \"value\": format_answer(item[\"answer\"])},\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # å¯«å…¥å‰å†æ¬¡é©—è­‰\n",
    "        line = json.dumps(record, ensure_ascii=False)\n",
    "        if has_hidden_newline(line):\n",
    "            print(f\"âš ï¸  è·³éç¬¬ {idx} ç­†ï¼šä»å«éš±è—æ›è¡Œç¬¦\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        processed.append(record)\n",
    "\n",
    "    print(f\"âœ” è½‰æ›å®Œæˆï¼š{len(processed)} ç­†ï¼ˆè·³é {skipped} ç­†ï¼‰\")\n",
    "\n",
    "    # éš¨æ©Ÿæ‰“äº‚\n",
    "    random.shuffle(processed)\n",
    "\n",
    "    # åˆ‡åˆ†\n",
    "    n        = len(processed)\n",
    "    n_train  = int(n * split[0])\n",
    "    n_val    = int(n * split[1])\n",
    "\n",
    "    splits = {\n",
    "        \"training.jsonl\":   processed[:n_train],\n",
    "        \"validation.jsonl\": processed[n_train:n_train + n_val],\n",
    "        \"test.jsonl\":       processed[n_train + n_val:],\n",
    "    }\n",
    "\n",
    "    # è¼¸å‡º\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    for fname, recs in splits.items():\n",
    "        path = Path(out_dir) / fname\n",
    "        with path.open(\"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "            for r in recs:\n",
    "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # å¯«å®Œå¾Œåšæ·±åº¦æƒæ\n",
    "        bad = deep_scan_jsonl(path)\n",
    "        if bad:\n",
    "            raise RuntimeError(f\"{fname} å­˜åœ¨éš±è—æ›è¡Œç¬¦ï¼Œå£è¡Œè™Ÿï¼š{bad[:10]}\")\n",
    "        print(f\"  â†’ {fname:<17} {len(recs)} è¡Œ  ï¼ˆé©—è­‰é€šéï¼‰\")\n",
    "\n",
    "    print(\"\\nğŸ‰  å…¨éƒ¨å®Œæˆï¼å¯ç›´æ¥äº¤çµ¦ NeMo é€²è¡Œ fine-tuneã€‚\")\n",
    "\n",
    "\n",
    "\n",
    "convert_dataset(SRC_JSON, OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020c3971-dcc5-4925-8a39-38ce01883a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/deep_ep-1.0.0+a84a248-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: lightning-utilities in /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg (0.14.0)\n",
      "Collecting lightning-utilities\n",
      "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.12/dist-packages (from lightning-utilities) (23.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities) (78.1.1)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities) (4.12.2)\n",
      "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: lightning-utilities\n",
      "  Attempting uninstall: lightning-utilities\n",
      "    Found existing installation: lightning-utilities 0.14.0\n",
      "    Uninstalling lightning-utilities-0.14.0:\n",
      "      Successfully uninstalled lightning-utilities-0.14.0\n",
      "Successfully installed lightning-utilities-0.14.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade lightning-utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad3ca60-739d-47b3-bc53-049f6e8d0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME']='/datasets/soc-20250703225140/'\n",
    "os.environ['HF_HUB_CACHE']='/datasets/soc-20250703225140/'\n",
    "os.environ['TRANSFORMERS_CACHE']='/datasets/soc-20250703225140/'\n",
    "os.environ['NEMO_HOME']='/datasets/soc-20250703225140/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ecbc84-fdc4-4728-b61d-321a79a3b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama-3_3-Nemotron-Super-49B-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb1f8d-63c6-4b6e-b88f-162af67d6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27c1cf86-4f2b-48f8-8ebb-e54c8ba3d2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-10 23:17:15 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> $</span><span style=\"color: #008000; text-decoration-color: #008000\">NEMO_MODELS_CACHE</span><span style=\"color: #008000; text-decoration-color: #008000\">=/datasets/soc-20250703225140/models </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m $\u001b[0m\u001b[32mNEMO_MODELS_CACHE\u001b[0m\u001b[32m=\u001b[0m\u001b[32m/datasets/soc-20250703225140/\u001b[0m\u001b[32mmodels\u001b[0m\u001b[32m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Imported Checkpoint</span>\n",
       "â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">context/</span>\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">artifacts/</span>\n",
       "â”‚   â”‚   â””â”€â”€ <span style=\"color: #808000; text-decoration-color: #808000\">generation_config.json</span>\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo_tokenizer/</span>\n",
       "â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #808000; text-decoration-color: #808000\">special_tokens_map.json</span>\n",
       "â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #808000; text-decoration-color: #808000\">tokenizer.json</span>\n",
       "â”‚   â”‚   â””â”€â”€ <span style=\"color: #808000; text-decoration-color: #808000\">tokenizer_config.json</span>\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #808000; text-decoration-color: #808000\">io.json</span>\n",
       "â”‚   â””â”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">model.yaml</span>\n",
       "â””â”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">weights/</span>\n",
       "    â”œâ”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">.metadata</span>\n",
       "    â”œâ”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">__0_0.distcp</span>\n",
       "    â”œâ”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">__0_1.distcp</span>\n",
       "    â”œâ”€â”€ <span style=\"color: #800080; text-decoration-color: #800080\">common.pt</span>\n",
       "    â””â”€â”€ <span style=\"color: #808000; text-decoration-color: #808000\">metadata.json</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mImported Checkpoint\u001b[0m\n",
       "â”œâ”€â”€ \u001b[1;36mcontext/\u001b[0m\n",
       "â”‚   â”œâ”€â”€ \u001b[1;36martifacts/\u001b[0m\n",
       "â”‚   â”‚   â””â”€â”€ \u001b[33mgeneration_config.json\u001b[0m\n",
       "â”‚   â”œâ”€â”€ \u001b[1;36mnemo_tokenizer/\u001b[0m\n",
       "â”‚   â”‚   â”œâ”€â”€ \u001b[33mspecial_tokens_map.json\u001b[0m\n",
       "â”‚   â”‚   â”œâ”€â”€ \u001b[33mtokenizer.json\u001b[0m\n",
       "â”‚   â”‚   â””â”€â”€ \u001b[33mtokenizer_config.json\u001b[0m\n",
       "â”‚   â”œâ”€â”€ \u001b[33mio.json\u001b[0m\n",
       "â”‚   â””â”€â”€ \u001b[37mmodel.yaml\u001b[0m\n",
       "â””â”€â”€ \u001b[1;36mweights/\u001b[0m\n",
       "    â”œâ”€â”€ \u001b[37m.metadata\u001b[0m\n",
       "    â”œâ”€â”€ \u001b[37m__0_0.distcp\u001b[0m\n",
       "    â”œâ”€â”€ \u001b[37m__0_1.distcp\u001b[0m\n",
       "    â”œâ”€â”€ \u001b[35mcommon.pt\u001b[0m\n",
       "    â””â”€â”€ \u001b[33mmetadata.json\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/datasets/soc-20250703225140/models/nvidia/Llama-3_3-Nemotron-Super-49B-v1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nemo.collections import llm\n",
    "llm.import_ckpt(model=llm.LlamaNemotronModel(llm.Llama33NemotronSuper49BConfig()), source='hf://nvidia/Llama-3_3-Nemotron-Super-49B-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51f28a-e3ce-4a13-8b33-c730da5f60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(llm.Llama33NemotronSuper49BConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b5e87d7-47ac-44ba-b1c5-d96cd6bb881c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (dir: Optional[str] = None, name: str = 'default', num_nodes: int = 1, num_gpus_per_node: int = 8, peft_scheme: Optional[str] = 'lora', seq_length: Optional[int] = None, packed_sequence: Optional[bool] = None, performance_mode: bool = False) -> nemo_run.config.Partial>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "from nemo.collections import llm\n",
    "\n",
    "inspect.signature(llm.llama33_nemotron_super_49b.finetune_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00262116-12a1-4b3c-b16b-bc4df4b8583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections import llm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "checkpoint_path=\"/datasets/soc-20250703225140/models--nvidia--Llama-3_3-Nemotron-Super-49B-v1\"\n",
    "\n",
    "\n",
    "recipe = llm.llama33_nemotron_super_49b.finetune_recipe(\n",
    "    name=\"llama33_nemotron_super_49b_finetuning\",\n",
    "    dir=checkpoint_path,\n",
    "    num_nodes=2,\n",
    "    num_gpus_per_node=1,\n",
    "    \n",
    "    peft_scheme='lora',  # 'lora', 'none'\n",
    "    packed_sequence=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade56a72-d00b-4af8-80b3-9c6369c092b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(recipe.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2941d93b-dc23-49a9-bef5-159685af4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast # å°å…¥ ast æ¨¡çµ„\n",
    "\n",
    "def read_qa_pairs_from_file() -> list:\n",
    "    \"\"\"\n",
    "    å¾æŒ‡å®šè·¯å¾‘çš„æª”æ¡ˆä¸­è®€å–å•é¡Œ-ç­”æ¡ˆé…å°ã€‚\n",
    "    å‡è¨­æª”æ¡ˆå…§å®¹æ˜¯ä¸€å€‹åŒ…å«å¤šå€‹å­—å…¸çš„ JSON é™£åˆ—ï¼Œæ¯å€‹å­—å…¸æœ‰ \"question\" å’Œ \"answer\" éµã€‚\n",
    "\n",
    "    Args:\n",
    "        file_path (str): è³‡æ–™æª”æ¡ˆçš„è·¯å¾‘ã€‚\n",
    "\n",
    "    Returns:\n",
    "        list: åŒ…å«å•é¡Œ-ç­”æ¡ˆå­—å…¸çš„åˆ—è¡¨ã€‚\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ã€‚\n",
    "        ValueError: å¦‚æœæª”æ¡ˆå…§å®¹ä¸æ˜¯é æœŸçš„åˆ—è¡¨æˆ–å­—å…¸æ ¼å¼ã€‚\n",
    "    \"\"\"\n",
    "    # ç§»é™¤é€™è£¡çš„è™›æ“¬è³‡æ–™æª”å‰µå»ºé‚è¼¯ï¼Œè®“å‡½å¼å°ˆæ³¨æ–¼è®€å–\n",
    "    # é€™å…©å€‹è·¯å¾‘æ‡‰è©²ä½œç‚ºåƒæ•¸å‚³å…¥ read_qa_pairs_from_file æˆ–åœ¨å¤–éƒ¨å®šç¾©\n",
    "    negative_sample_path = \"/datasets/soc-20250703225140/dataset/negative_sample.txt\"\n",
    "    creative_sample_path = \"/datasets/soc-20250703225140/dataset/creative_dataset.txt\"\n",
    "    \n",
    "    negative_list = []\n",
    "    creative_dataset = None\n",
    "    \n",
    "    # å¾ negative_sample.txt è®€å–ï¼Œä½¿ç”¨ ast.literal_eval\n",
    "    with open(negative_sample_path, 'r', encoding='utf-8') as f:\n",
    "        negative_list = ast.literal_eval(f.read())\n",
    "    \n",
    "    # å¾ creative_dataset.txt è®€å–ï¼Œä½¿ç”¨ json.load\n",
    "    with open(creative_sample_path, \"r\", encoding='utf-8') as f:\n",
    "        creative_dataset = json.load(f)\n",
    "\n",
    "    creative_list = []\n",
    "    for creative_data_item in creative_dataset: # ä¿®æ”¹è®Šæ•¸åä»¥é¿å…æ··æ·†\n",
    "        # å‡è¨­ creative_data_item æ˜¯ {'key': ['qa1', 'qa2']} çš„å½¢å¼\n",
    "        for key, value_list in creative_data_item.items():\n",
    "            creative_list.extend(value_list) # ä½¿ç”¨ extend æ·»åŠ åˆ—è¡¨ä¸­çš„æ‰€æœ‰å…ƒç´ \n",
    "        \n",
    "    qa_pairs = negative_list + creative_list\n",
    "\n",
    "    if not isinstance(qa_pairs, list) or not all(isinstance(item, dict) and \"question\" in item and \"answer\" in item for item in qa_pairs):\n",
    "        raise ValueError(\"æª”æ¡ˆå…§å®¹ä¸æ˜¯æœ‰æ•ˆçš„ Q&A å­—å…¸åˆ—è¡¨æ ¼å¼ã€‚è«‹ç¢ºèªæ¯å€‹é …ç›®éƒ½æ˜¯åŒ…å« 'question' å’Œ 'answer' éµçš„å­—å…¸ã€‚\")\n",
    "\n",
    "    return qa_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64387024-416f-4f87-8926-7487c5e0c604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the CPIC-recommended dose for metformin in a patient with the CYP2D6 *4/*4 genotype?',\n",
       " 'answer': 'No CPIC guideline information available.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57759a95-c7a6-4121-9b41-d353135859d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creative_data[0]\n",
    "# creative_list = []\n",
    "# for creative_data in creative_dataset:\n",
    "#     for _, value in creative_data.items():\n",
    "#         creative_list += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6a0f7ed-f35a-43f7-b1d7-f5bb910a383c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the recommended ivacaftor dose for a patient with the CFTR G551D mutation?',\n",
       " 'answer': {'Drug Name': 'ivacaftor',\n",
       "  'Gene Name': 'CFTR',\n",
       "  'CPIC Guideline Name': 'Clinical Pharmacogenetics Implementation Consortium (CPIC) Guidelines for Ivacaftor Therapy in the Context of CFTR Genotype (March 2014).pdf',\n",
       "  'Content to Search': 'Recommended ivacaftor dosage for patients with the CFTR G551D genotype.'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creative_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9fdf572-f3c8-4391-a594-477571c630d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_dataset = creative_list + negative_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e956ee-aacb-40da-b8dc-b21b4508cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer # å‡è¨­ä½ æœƒç”¨ Hugging Face çš„åˆ†è©å™¨\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs: list, tokenizer: AutoTokenizer, max_length: int = 512):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è‡ªå®šç¾©çš„å•ç­”è³‡æ–™é›†ã€‚\n",
    "\n",
    "        Args:\n",
    "            qa_pairs (list): å¾æª”æ¡ˆè®€å–çš„å•é¡Œ-ç­”æ¡ˆå­—å…¸åˆ—è¡¨ã€‚\n",
    "            tokenizer (AutoTokenizer): ç”¨æ–¼è™•ç†æ–‡æœ¬çš„ Hugging Face åˆ†è©å™¨ã€‚\n",
    "            max_length (int): åºåˆ—çš„æœ€å¤§é•·åº¦ï¼Œç”¨æ–¼åˆ†è©å™¨çš„æˆªæ–·ã€‚\n",
    "        \"\"\"\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # è¿”å›è³‡æ–™é›†ä¸­çš„å•é¡Œ-ç­”æ¡ˆå°çš„ç¸½æ•¸\n",
    "        return len(self.qa_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # æ ¹æ“šç´¢å¼•ç²å–ä¸€å€‹å–®å€‹æ¨£æœ¬\n",
    "        item = self.qa_pairs[idx]\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "\n",
    "        # é€™è£¡è¿”å›åŸå§‹å­—ä¸²ï¼Œåˆ†è©å’Œå¡«å……çš„æ­¥é©Ÿå°‡åœ¨ `collate_fn` ä¸­è™•ç†ã€‚\n",
    "        return {\"question\": question, \"answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21c606d-5892-42bd-b201-f0df9f5d607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "def custom_collate_fn(batch: list, tokenizer: AutoTokenizer, max_length: int):\n",
    "    \"\"\"\n",
    "    è‡ªå®šç¾©çš„æ‰¹æ¬¡åŒ–å‡½å¼ã€‚å°‡å¤šå€‹ Q&A æ¨£æœ¬è™•ç†æˆæ¨¡å‹å¯æ¥å—çš„å–®å€‹æ‰¹æ¬¡ã€‚\n",
    "\n",
    "    Args:\n",
    "        batch (list): åŒ…å«å¤šå€‹ `__getitem__` è¿”å›çš„å­—å…¸ï¼ˆä¾‹å¦‚ [{\"question\": \"...\", \"answer\": \"...\"}, ...]ï¼‰ã€‚\n",
    "        tokenizer (AutoTokenizer): Hugging Face åˆ†è©å™¨ã€‚\n",
    "        max_length (int): åºåˆ—çš„æœ€å¤§é•·åº¦ã€‚\n",
    "\n",
    "    Returns:\n",
    "        dict: åŒ…å« 'input_ids', 'attention_mask' å’Œ 'labels' çš„å­—å…¸ï¼Œé©åˆæ¨¡å‹è¨“ç·´ã€‚\n",
    "    \"\"\"\n",
    "    questions = [item[\"question\"] for item in batch]\n",
    "    answers = [item[\"answer\"] for item in batch]\n",
    "\n",
    "    # å°‡å•é¡Œå’Œç­”æ¡ˆçµ„åˆæˆä¸€å€‹ç”¨æ–¼å› æœèªè¨€å»ºæ¨¡çš„è¨“ç·´æ ¼å¼ã€‚\n",
    "    # é€™ç¨®æ ¼å¼å¸¸è¦‹æ–¼å¾®èª¿æ¨¡å‹ä»¥ç”Ÿæˆç­”æ¡ˆã€‚\n",
    "    # é€šå¸¸çš„æ ¼å¼ç‚ºï¼š\"å•é¡Œ: [ä½ çš„å•é¡Œ]\\nç­”æ¡ˆ: [ä½ çš„ç­”æ¡ˆ]<eos_token>\"\n",
    "    texts_to_tokenize = [f\"å•é¡Œ: {q}\\nç­”æ¡ˆ: {a}{tokenizer.eos_token}\" for q, a in zip(questions, answers)]\n",
    "\n",
    "    # å°æ•´å€‹æ‰¹æ¬¡çš„æ–‡æœ¬é€²è¡Œåˆ†è©å’Œå¡«å……\n",
    "    tokenized_batch = tokenizer(\n",
    "        texts_to_tokenize,\n",
    "        max_length=max_length, # å°‡æ‰€æœ‰åºåˆ—å¡«å……æˆ–æˆªæ–·åˆ°é€™å€‹é•·åº¦\n",
    "        truncation=True,       # å¦‚æœåºåˆ—è¶…é max_lengthï¼Œå‰‡æˆªæ–·\n",
    "        padding=\"max_length\",  # å¡«å……åˆ° max_length\n",
    "        return_tensors=\"pt\"    # è¿”å› PyTorch å¼µé‡\n",
    "    )\n",
    "\n",
    "    # å°æ–¼å› æœèªè¨€æ¨¡å‹ (Causal Language Model)ï¼Œé€šå¸¸ `input_ids` ä¹Ÿç”¨ä½œ `labels`ã€‚\n",
    "    # æ¨¡å‹æœƒåœ¨å…§éƒ¨å°‡ `labels` åç§»ä¸€å€‹ä½ç½®ä¾†é€²è¡Œä¸‹ä¸€å€‹è©çš„é æ¸¬ã€‚\n",
    "    # æˆ‘å€‘è¤‡è£½ä¸€ä»½ï¼Œå› ç‚º labels å¯èƒ½æœƒè¢«ä¿®æ”¹ (ä¾‹å¦‚ -100 å¡«å……)ã€‚\n",
    "    labels = tokenized_batch[\"input_ids\"].clone()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_batch[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_batch[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23288e79-630d-487f-b84d-bced9646a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_function_that_configures_your_custom_dataset(gbs: int, mbs: int, seq_length: int, tokenizer: AutoTokenizer) -> DataLoader:\n",
    "    \"\"\"\n",
    "    é…ç½®ä¸¦è¿”å›ä¸€å€‹ç”¨æ–¼ NeMo Recipe çš„å®¢è£½åŒ– DataLoaderã€‚\n",
    "\n",
    "    Args:\n",
    "        gbs (int): Global Batch Size (ç¸½æ‰¹æ¬¡å¤§å°)ã€‚\n",
    "        mbs (int): Micro Batch Size (æ¯å€‹ GPU çš„æ‰¹æ¬¡å¤§å°)ã€‚é€™å°‡ä½œç‚º DataLoader çš„ batch_sizeã€‚\n",
    "        seq_length (int): åºåˆ—æœ€å¤§é•·åº¦ã€‚\n",
    "        tokenizer (AutoTokenizer): Hugging Face åˆ†è©å™¨ã€‚\n",
    "        qa_file_path (str): ä½ çš„ Q&A æ•¸æ“šæª”æ¡ˆçš„è·¯å¾‘ã€‚\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: é…ç½®å¥½çš„æ•¸æ“šè¼‰å…¥å™¨ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"\\nConfiguring DataLoader with: Global Batch Size={gbs}, Micro Batch Size={mbs}, Sequence Length={seq_length}\")\n",
    "    \n",
    "    # è®€å–ä½ çš„ Q&A æ•¸æ“š\n",
    "    qa_pairs = read_qa_pairs_from_file()\n",
    "    \n",
    "    # å¯¦ä¾‹åŒ–ä½ çš„å®¢è£½åŒ–æ•¸æ“šé›†\n",
    "    custom_dataset_instance = QADataset(qa_pairs=qa_pairs, tokenizer=tokenizer, max_length=seq_length)\n",
    "    print(f\"Custom dataset created with {len(custom_dataset_instance)} samples.\")\n",
    "    \n",
    "    # å‰µå»º DataLoader å¯¦ä¾‹\n",
    "    # DataLoader çš„ batch_size åƒæ•¸ç›´æ¥å°æ‡‰æ¯å€‹ GPU çš„ micro batch size (mbs)\n",
    "    custom_dataloader = DataLoader(\n",
    "        custom_dataset_instance,\n",
    "        batch_size=mbs,\n",
    "        shuffle=True, # è¨“ç·´æ™‚é€šå¸¸éœ€è¦æ‰“äº‚æ•¸æ“š\n",
    "        num_workers=0, # åœ¨ Jupyter/Colab ç’°å¢ƒä¸­ï¼Œ0 é€šå¸¸æ˜¯å®‰å…¨çš„é¸æ“‡\n",
    "        pin_memory=True, # å•Ÿç”¨ pin_memory å¯ä»¥åŠ é€Ÿæ•¸æ“šå‚³è¼¸åˆ° GPU\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer=tokenizer, max_length=seq_length)\n",
    "    )\n",
    "    print(f\"DataLoader created with {len(custom_dataloader)} batches (each of size {mbs}).\")\n",
    "    return custom_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b1f75a-1791-43b9-84ab-eab7ec680f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†è©å™¨çš„ pad_token å·²è¨­å®šç‚º eos_token: 128009\n",
      "\n",
      "Configuring DataLoader with: Global Batch Size=8, Micro Batch Size=4, Sequence Length=2048\n",
      "Custom dataset created with 4391 samples.\n",
      "DataLoader created with 1098 batches (each of size 4).\n"
     ]
    }
   ],
   "source": [
    "# --- ç¯„ä¾‹ä½¿ç”¨æ–¹å¼ (æ¨¡æ“¬ NeMo Recipe å’Œ run.run) ---\n",
    "# if __name__ == \"__main__\":\n",
    "# --- å¿…è¦è¨­å®šèˆ‡æª”æ¡ˆå®šç¾© ---\n",
    "\n",
    "# è¨­å®šå¿«å–ç›®éŒ„ (ç¢ºä¿æœ‰å¯«å…¥æ¬Šé™)\n",
    "\n",
    "cache_dir = '/datasets/soc-20250703225140/'\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_HUB_CACHE'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['NEMO_HOME']=cache_dir\n",
    "\"\"\"\n",
    "export HF_HOME=/datasets/soc-20250703225140/\n",
    "export HF_HUB_CACHE=/datasets/soc-20250703225140/\n",
    "export TRANSFORMERS_CACHE=/datasets/soc-20250703225140/\n",
    "export NEMO_HOME=/datasets/soc-20250703225140/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# --- è¼‰å…¥åˆ†è©å™¨ ---\n",
    "model_name=\"nvidia/Llama-3_3-Nemotron-Super-49B-v1\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"åˆ†è©å™¨çš„ pad_token å·²è¨­å®šç‚º eos_token: {tokenizer.pad_token_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥åˆ†è©å™¨ '{tokenizer_model_id}'ã€‚{e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- æ¨¡æ“¬ NeMo Recipe åƒæ•¸ ---\n",
    "# é€™äº›åƒæ•¸é€šå¸¸å¾ NeMo Recipe æˆ–å…¶æ¨¡å‹é…ç½®ä¸­ç²å–\n",
    "# ç¢ºä¿é€™äº›å€¼èˆ‡ä½ çš„å¯¦éš›è¨“ç·´é…ç½®ç›¸ç¬¦\n",
    "gbs_param = 8   # Global Batch Size\n",
    "mbs_param = 4   # Micro Batch Size per GPU\n",
    "\n",
    "\n",
    "# --- å‘¼å« a_function_that_configures_your_custom_dataset ä¾†ç²å– DataLoader ---\n",
    "dataloader = a_function_that_configures_your_custom_dataset(\n",
    "    gbs=gbs_param,\n",
    "    mbs=mbs_param,\n",
    "    seq_length=recipe.model.config.seq_length,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9510c010-1dab-4461-89c2-db4362b1c4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†è©å™¨çš„ pad_token å·²è¨­å®šç‚º eos_token: 128009\n",
      "\n",
      "Configuring DataLoader with: Global Batch Size=8, Micro Batch Size=4, Sequence Length=2048\n",
      "Custom dataset created with 4391 samples.\n",
      "DataLoader created with 1098 batches (each of size 4).\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/datasets/soc-20250703225140/'\n",
    "# os.makedirs(my_hf_cache_dir, exist_ok=True)\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_HUB_CACHE'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['NEMO_HOME']=cache_dir\n",
    "\"\"\"\n",
    "export HF_HOME=/datasets/soc-20250703225140/\n",
    "export HF_HUB_CACHE=/datasets/soc-20250703225140/\n",
    "export TRANSFORMERS_CACHE=/datasets/soc-20250703225140/\n",
    "export NEMO_HOME=/datasets/soc-20250703225140/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# --- è¼‰å…¥åˆ†è©å™¨ ---\n",
    "model_name=\"nvidia/Llama-3_3-Nemotron-Super-49B-v1\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"åˆ†è©å™¨çš„ pad_token å·²è¨­å®šç‚º eos_token: {tokenizer.pad_token_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥åˆ†è©å™¨ '{tokenizer_model_id}'ã€‚{e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- æ¨¡æ“¬ NeMo Recipe åƒæ•¸ ---\n",
    "# é€™äº›åƒæ•¸é€šå¸¸å¾ NeMo Recipe æˆ–å…¶æ¨¡å‹é…ç½®ä¸­ç²å–\n",
    "# ç¢ºä¿é€™äº›å€¼èˆ‡ä½ çš„å¯¦éš›è¨“ç·´é…ç½®ç›¸ç¬¦\n",
    "gbs_param = 8   # Global Batch Size\n",
    "mbs_param = 4   # Micro Batch Size per GPU\n",
    "\n",
    "\n",
    "# --- å‘¼å« a_function_that_configures_your_custom_dataset ä¾†ç²å– DataLoader ---\n",
    "dataloader = a_function_that_configures_your_custom_dataset(\n",
    "    gbs=gbs_param,\n",
    "    mbs=mbs_param,\n",
    "    seq_length=recipe.model.config.seq_length,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d73d686-a696-4085-a52a-3eda94e0f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring DataLoader with: Global Batch Size=8, Micro Batch Size=4, Sequence Length=2048\n",
      "Custom dataset created with 4391 samples.\n",
      "DataLoader created with 1098 batches (each of size 4).\n"
     ]
    }
   ],
   "source": [
    "dataloader = a_function_that_configures_your_custom_dataset(\n",
    "    gbs=gbs_param,\n",
    "    mbs=mbs_param,\n",
    "    tokenizer=tokenizer,\n",
    "    seq_length=recipe.model.config.seq_length,\n",
    ")\n",
    "recipe.data = dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a33168-f97c-4530-92c5-4ce66e261dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1751990357</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92mâ”€â”€â”€ \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1751990357\u001b[0m\u001b[92m â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnserializableValueError",
     "evalue": "Unserializable value <torch.utils.data.dataloader.DataLoader object at 0x7f18c47ebce0> of type <class 'torch.utils.data.dataloader.DataLoader'>. Error occurred at path '<root>.data'.\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnserializableValueError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m recipe.trainer.strategy.tensor_model_parallel_size = \u001b[32m2\u001b[39m\n\u001b[32m      4\u001b[39m recipe.trainer.strategy.pipeline_model_parallel_size = \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLocalExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/run/api.py:80\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(fn_or_script, executor, plugins, name, dryrun, direct, detach, tail_logs, log_level)\u001b[39m\n\u001b[32m     78\u001b[39m name = name \u001b[38;5;129;01mor\u001b[39;00m default_name\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Experiment(title=name, executor=executor, log_level=log_level) \u001b[38;5;28;01mas\u001b[39;00m exp:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn_or_script\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_logs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtail_logs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dryrun:\n\u001b[32m     82\u001b[39m         exp.dryrun()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/run/experiment.py:542\u001b[39m, in \u001b[36mExperiment.add\u001b[39m\u001b[34m(self, task, executor, name, plugins, tail_logs, dependencies)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m executor \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor, Executor)\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     job_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_single_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtail_logs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtail_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m name, \u001b[33m\"\u001b[39m\u001b[33mname is required for task group.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/run/experiment.py:421\u001b[39m, in \u001b[36mExperiment._add_single_job\u001b[39m\u001b[34m(self, task, executor, name, plugins, tail_logs, dependencies)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    419\u001b[39m     task_id = name\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m executor = executor.clone()\n\u001b[32m    424\u001b[39m executor.assign(\n\u001b[32m    425\u001b[39m     \u001b[38;5;28mself\u001b[39m._id,\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m._exp_dir,\n\u001b[32m    427\u001b[39m     task_id=task_id,\n\u001b[32m    428\u001b[39m     task_dir=name \u001b[38;5;28;01mif\u001b[39;00m reuse_job_dir \u001b[38;5;28;01melse\u001b[39;00m task_id,\n\u001b[32m    429\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/run/experiment.py:500\u001b[39m, in \u001b[36mExperiment._validate_task\u001b[39m\u001b[34m(self, task_info, task)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, Partial):\n\u001b[32m    499\u001b[39m     serializer = ZlibJSONSerializer()\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     serialized = \u001b[43mserializer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m     deserialized = serializer.deserialize(serialized)\n\u001b[32m    502\u001b[39m     diff = diffing.build_diff(deserialized, task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/core/serialization/zlib_json.py:33\u001b[39m, in \u001b[36mZlibJSONSerializer.serialize\u001b[39m\u001b[34m(self, cfg, pyref_policy)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mserialize\u001b[39m(\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     29\u001b[39m     cfg: config.Buildable,\n\u001b[32m     30\u001b[39m     pyref_policy: Optional[serialization.PyrefPolicy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     31\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m base64.urlsafe_b64encode(\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         zlib.compress(\u001b[43mserialization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyref_policy\u001b[49m\u001b[43m)\u001b[49m.encode())\n\u001b[32m     34\u001b[39m     ).decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/fiddle/_src/experimental/serialization.py:826\u001b[39m, in \u001b[36mdump_json\u001b[39m\u001b[34m(value, pyref_policy, indent)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump_json\u001b[39m(\n\u001b[32m    803\u001b[39m     value: Any,\n\u001b[32m    804\u001b[39m     pyref_policy: Optional[PyrefPolicy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    805\u001b[39m     indent: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    806\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    807\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns the JSON serialization of `value`.\u001b[39;00m\n\u001b[32m    808\u001b[39m \n\u001b[32m    809\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m \u001b[33;03m      disallowed by `pyref_policy`.\u001b[39;00m\n\u001b[32m    825\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m json.dumps(\u001b[43mSerialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyref_policy\u001b[49m\u001b[43m)\u001b[49m.result, indent=indent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/fiddle/_src/experimental/serialization.py:527\u001b[39m, in \u001b[36mSerialization.__init__\u001b[39m\u001b[34m(self, value, pyref_policy)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28mself\u001b[39m._pyref_policy = pyref_policy \u001b[38;5;129;01mor\u001b[39;00m DefaultPyrefPolicy()\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# The result of the serialization.\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[38;5;28mself\u001b[39m._result = {\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     _ROOT_KEY: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    528\u001b[39m     _OBJECTS_KEY: \u001b[38;5;28mself\u001b[39m._objects,\n\u001b[32m    529\u001b[39m     _REFCOUNTS_KEY: \u001b[38;5;28mself\u001b[39m._refcounts,\n\u001b[32m    530\u001b[39m     _VERSION_KEY: _VERSION,\n\u001b[32m    531\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo/nemo/lightning/io/fdl_torch.py:134\u001b[39m, in \u001b[36menable.<locals>._modified_serialize\u001b[39m\u001b[34m(self, value, current_path, all_paths)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, types.BuiltinFunctionType):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pyref(value, current_path)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_original_serialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/fiddle/_src/experimental/serialization.py:662\u001b[39m, in \u001b[36mSerialization._serialize\u001b[39m\u001b[34m(self, value, current_path, all_paths)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path_element, child_value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(path_elements, values):\n\u001b[32m    659\u001b[39m   child_paths = (\n\u001b[32m    660\u001b[39m       daglish.add_path_element(all_paths, path_element)\n\u001b[32m    661\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m all_paths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m   serialized_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m      \u001b[49m\u001b[43mchild_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_element\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchild_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m   \u001b[38;5;66;03m# The serialized item is a two-element list with a string representation\u001b[39;00m\n\u001b[32m    665\u001b[39m   \u001b[38;5;66;03m# of the path element available for debugging/visualization purposes.\u001b[39;00m\n\u001b[32m    666\u001b[39m   serialized_item = (\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_element\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m, serialized_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo/nemo/lightning/io/fdl_torch.py:134\u001b[39m, in \u001b[36menable.<locals>._modified_serialize\u001b[39m\u001b[34m(self, value, current_path, all_paths)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, types.BuiltinFunctionType):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pyref(value, current_path)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_original_serialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/fiddle/_src/experimental/serialization.py:651\u001b[39m, in \u001b[36mSerialization._serialize\u001b[39m\u001b[34m(self, value, current_path, all_paths)\u001b[39m\n\u001b[32m    648\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    649\u001b[39m     msg = (\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnserializable value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Error \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    650\u001b[39m            \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33moccurred at path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_str(current_path)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnserializableValueError(msg)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# We have a traverser; serialize value's flattened elements.\u001b[39;00m\n\u001b[32m    653\u001b[39m   values, metadata = traverser.flatten(value)\n",
      "\u001b[31mUnserializableValueError\u001b[39m: Unserializable value <torch.utils.data.dataloader.DataLoader object at 0x7f18c47ebce0> of type <class 'torch.utils.data.dataloader.DataLoader'>. Error occurred at path '<root>.data'.\")"
     ]
    }
   ],
   "source": [
    "import nemo_run as run\n",
    "\n",
    "recipe.trainer.strategy.tensor_model_parallel_size = 2\n",
    "recipe.trainer.strategy.pipeline_model_parallel_size = 1\n",
    "\n",
    "run.run(recipe, executor=run.LocalExecutor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bbf2a-7487-43e1-b5e0-c883bc456b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nemo.collections import llm\n",
    "import nemo_run as run\n",
    "from nemo.collections.common.data.datamodule import DataModule as NeMoDataModule\n",
    "\n",
    "# --- 0. Setup: Define necessary paths and ensure directories exist ---\n",
    "my_hf_cache_dir = \"/datasets/soc-20250703225140/\" # Your specified cache dir\n",
    "os.makedirs(my_hf_cache_dir, exist_ok=True)\n",
    "os.environ['HF_HOME'] = my_hf_cache_dir\n",
    "os.environ['HF_HUB_CACHE'] = my_hf_cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = my_hf_cache_dir\n",
    "os.environ['NEMO_HOME'] = my_hf_cache_dir\n",
    "\n",
    "checkpoint_dir = \"/datasets/soc-20250703225140/nemo_checkpoints\" # Example, ensure this exists\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "qa_data_file = \"/datasets/soc-20250703225140/dataset/negative_sample.txt\" # Your data file path\n",
    "\n",
    "# --- 1. Custom Dataset Class ---\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs: list, tokenizer: AutoTokenizer, max_length: int = 512):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.qa_pairs[idx]\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        return {\"question\": question, \"answer\": answer}\n",
    "\n",
    "# --- 2. Custom Collate Function ---\n",
    "def collate_fn(batch: list, tokenizer: AutoTokenizer, max_length: int):\n",
    "    questions = [item[\"question\"] for item in batch]\n",
    "    answers = [item[\"answer\"] for item in batch]\n",
    "    texts_to_tokenize = [f\"å•é¡Œ: {q}\\nç­”æ¡ˆ: {a}{tokenizer.eos_token}\" for q, a in zip(questions, answers)]\n",
    "    tokenized_batch = tokenizer(\n",
    "        texts_to_tokenize,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenized_batch[\"input_ids\"].clone()\n",
    "    return {\n",
    "        \"input_ids\": tokenized_batch[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_batch[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# --- 3. Read QA Pairs From File (Adjusted for your specific files) ---\n",
    "def read_qa_pairs_from_file() -> list: # Removed file_path arg as paths are hardcoded\n",
    "    negative_sample_path = \"/datasets/soc-20250703225140/dataset/negative_sample.txt\"\n",
    "    creative_sample_path = \"/datasets/soc-20250703225140/dataset/creative_dataset.txt\" # Assuming this is JSON\n",
    "\n",
    "    negative_list = []\n",
    "    creative_dataset = None\n",
    "\n",
    "    # Read negative_sample.txt (assumed to be Python list string)\n",
    "    if os.path.exists(negative_sample_path):\n",
    "        with open(negative_sample_path, 'r', encoding='utf-8') as f:\n",
    "            negative_list = ast.literal_eval(f.read())\n",
    "    else:\n",
    "        print(f\"Warning: {negative_sample_path} not found. Skipping negative samples.\")\n",
    "\n",
    "    # Read creative_dataset.txt (assumed to be JSON)\n",
    "    if os.path.exists(creative_sample_path):\n",
    "        with open(creative_sample_path, \"r\", encoding='utf-8') as f:\n",
    "            creative_dataset = json.load(f)\n",
    "    else:\n",
    "        print(f\"Warning: {creative_sample_path} not found. Skipping creative dataset.\")\n",
    "        creative_dataset = []\n",
    "\n",
    "    creative_list = []\n",
    "    if creative_dataset:\n",
    "        for creative_data_item in creative_dataset:\n",
    "            for _, value_list in creative_data_item.items():\n",
    "                creative_list.extend(value_list)\n",
    "        \n",
    "    qa_pairs = negative_list + creative_list\n",
    "\n",
    "    if not isinstance(qa_pairs, list) or not all(isinstance(item, dict) and \"question\" in item and \"answer\" in item for item in qa_pairs):\n",
    "        raise ValueError(\"Combined data is not a valid list of Q&A dictionaries.\")\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "# --- 4. Custom DataModule (as previously defined) ---\n",
    "class CustomQAPairDataModule(NeMoDataModule):\n",
    "    def __init__(self,\n",
    "                 qa_file_path: str, # Now takes the main combined QA file path\n",
    "                 tokenizer: AutoTokenizer, # Still pass tokenizer instance\n",
    "                 max_length: int,\n",
    "                 gbs: int,\n",
    "                 mbs: int,\n",
    "                 num_workers: int = 0,\n",
    "                 shuffle: bool = True,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.qa_file_path = qa_file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.gbs = gbs\n",
    "        self.mbs = mbs\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.qa_pairs = None # Data will be loaded in setup\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        # Data loading now happens within the data module's setup\n",
    "        # It's better to pass the full list of Q&A pairs to the DataModule init\n",
    "        # then to load it inside, if the list is already constructed in the main script.\n",
    "        # But for this example, we keep the file path here.\n",
    "        self.qa_pairs = read_qa_pairs_from_file() # Calls the global function\n",
    "        print(f\"[DataModule] Loaded {len(self.qa_pairs)} QA pairs in setup stage.\")\n",
    "        \n",
    "        self.train_dataset = QADataset(\n",
    "            qa_pairs=self.qa_pairs,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        custom_collater = lambda batch: collate_fn(batch, tokenizer=self.tokenizer, max_length=self.max_length)\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.mbs,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=custom_collater,\n",
    "        )\n",
    "\n",
    "# --- 5. Function to configure DataModule CFG ---\n",
    "def a_function_that_configures_your_custom_dataset_cfg(gbs: int, mbs: int, seq_length: int, tokenizer_model_id: str, qa_file_path: str) -> dict:\n",
    "    return {\n",
    "        \"_target_\": f\"{__name__}.CustomQAPairDataModule\",\n",
    "        \"qa_file_path\": qa_file_path,\n",
    "        \"tokenizer\": {\"_target_\": \"transformers.AutoTokenizer.from_pretrained\", \"pretrained_model_name_or_path\": tokenizer_model_id, \"trust_remote_code\": True, \"cache_dir\": os.getenv('HF_HOME')}, # Ensure tokenizer loads to correct cache\n",
    "        \"max_length\": seq_length,\n",
    "        \"gbs\": gbs,\n",
    "        \"mbs\": mbs,\n",
    "        \"num_workers\": 0,\n",
    "        \"shuffle\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    import ast # Need to import ast here as well if not already\n",
    "    \n",
    "    # --- Necessary setup and file definitions ---\n",
    "    cache_dir = '/datasets/soc-20250703225140/'\n",
    "    # os.makedirs(cache_dir, exist_ok=True) # Assuming this is handled by PrimeHub\n",
    "    os.environ['HF_HOME'] = cache_dir\n",
    "    os.environ['HF_HUB_CACHE'] = cache_dir\n",
    "    os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "    os.environ['NEMO_HOME'] = cache_dir\n",
    "    print(f\"Hugging Face and NeMo cache directory set to: {cache_dir}\")\n",
    "\n",
    "    checkpoint_path = \"/datasets/soc-20250703225140/nemo_checkpoints\"\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    # --- Load Tokenizer ---\n",
    "    model_name = \"nvidia/Llama-3_3-Nemotron-Super-49B-v1\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, cache_dir=cache_dir)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(f\"åˆ†è©å™¨çš„ pad_token å·²è¨­å®šç‚º eos_token: {tokenizer.pad_token_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥åˆ†è©å™¨ '{model_name}'ã€‚è«‹æª¢æŸ¥ç¶²è·¯é€£ç·šæˆ– HFTokenã€‚éŒ¯èª¤: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # --- Simulate NeMo Recipe Parameters ---\n",
    "    # These parameters typically come from the NeMo Recipe or its model configuration\n",
    "    num_nodes_actual = 1       # You have 1 node\n",
    "    num_gpus_per_node_actual = 2 # You have 2 A100 80GB GPUs on this node\n",
    "\n",
    "    global_batch_size_recipe = 8   # Global Batch Size\n",
    "    micro_batch_size_per_gpu_recipe = 4 # Micro Batch Size per GPU (adjust based on OOM tests)\n",
    "\n",
    "    # You can specify this directly or derive from a model config if available\n",
    "    model_seq_length_from_recipe = 2048 # Example: 2048 or 4096, adjust based on model/GPU\n",
    "\n",
    "    # --- Instantiate NeMo Recipe ---\n",
    "    recipe = llm.llama33_nemotron_super_49b.finetune_recipe(\n",
    "        name=\"llama33_nemotron_super_49b_finetuning\",\n",
    "        dir=checkpoint_path,\n",
    "        num_nodes=num_nodes_actual,\n",
    "        num_gpus_per_node=num_gpus_per_node_actual,\n",
    "        peft_scheme='lora',\n",
    "        packed_sequence=False,\n",
    "        global_batch_size=global_batch_size_recipe,\n",
    "        micro_batch_size_per_gpu=micro_batch_size_per_gpu_recipe,\n",
    "        seq_length=model_seq_length_from_recipe,\n",
    "    )\n",
    "    print(\"\\nNeMo Recipe å¯¦ä¾‹å·²å‰µå»ºã€‚\")\n",
    "\n",
    "    # --- CRITICAL FIX: Set Model Parallelism Parameters ---\n",
    "    # These parameters must be set AFTER the recipe is instantiated\n",
    "    # but BEFORE it is run.\n",
    "    # For 2 GPUs, TP=2, PP=1 is the only valid combination for optimal use.\n",
    "    recipe.trainer.strategy.tensor_model_parallel_size = 2\n",
    "    recipe.trainer.strategy.pipeline_model_parallel_size = 1 # Assuming no pipeline parallelism\n",
    "    # If context parallelism is implicitly used, ensure its size * (TP*PP) divides total GPUs.\n",
    "    # For Llama 3 models, context_parallel_size is often 1.\n",
    "\n",
    "    print(f\"è¨­å®šæ¨¡å‹ä¸¦è¡Œåƒæ•¸: tensor_model_parallel_size={recipe.trainer.strategy.tensor_model_parallel_size}, pipeline_model_parallel_size={recipe.trainer.strategy.pipeline_model_parallel_size}\")\n",
    "\n",
    "\n",
    "    # --- Override the data argument ---\n",
    "    custom_datamodule_cfg = a_function_that_configures_your_custom_dataset_cfg(\n",
    "        gbs=global_batch_size_recipe,\n",
    "        mbs=micro_batch_size_per_gpu_recipe,\n",
    "        seq_length=model_seq_length_from_recipe,\n",
    "        tokenizer_model_id=model_name, # Pass tokenizer_model_id, not the tokenizer instance\n",
    "        qa_file_path=qa_data_file\n",
    "    )\n",
    "    recipe.data = custom_datamodule_cfg\n",
    "    print(\"NeMo Recipe's data argument has been successfully overridden with your custom DataModule configuration.\")\n",
    "\n",
    "    # --- Run the NeMo Recipe ---\n",
    "    print(\"\\nå˜—è©¦é‹è¡Œ NeMo Recipe...\")\n",
    "    try:\n",
    "        run.run(recipe, executor=run.LocalExecutor())\n",
    "        print(\"\\nNeMo Recipe åŸ·è¡ŒæˆåŠŸï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\né‹è¡Œ NeMo Recipe æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "        print(\"è«‹æª¢æŸ¥æ‚¨çš„ NeMo é…ç½®ã€æ•¸æ“šè·¯å¾‘ã€ä»¥åŠ GPU è³‡æºæ˜¯å¦åŒ¹é…ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
