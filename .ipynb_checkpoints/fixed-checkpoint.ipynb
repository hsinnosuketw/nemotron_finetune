{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb342b1-8c7a-4914-9f17-d5643fdec5fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3747645088.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from nemo.lightning import AutoResume\n",
    "\n",
    "resume = AutoResume(\n",
    "    resume_if_exists=True,\n",
    "    resume_ignore_no_cㄇheckpoint=False,\n",
    "    resume_from_directory=\"/datasets/soc-20250703225140/nemo_checkpoints/nemotron_49b_super_custom_finetune/2025-07-14_17-42-12/checkpoints/model_name=0--val_loss=0.03-step=449-consumed_samples=900.0-last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e0bfa-0bfd-4d1b-838f-b30fe49d9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nemo_inference.py\n",
    "\n",
    "import torch.distributed\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "import nemo.lightning as nl\n",
    "import re\n",
    "\n",
    "strategy = nl.MegatronStrategy(\n",
    "    tensor_model_parallel_size=2,\n",
    "    pipeline_model_parallel_size=1,\n",
    "    context_parallel_size=1,\n",
    "    sequence_parallel=False,\n",
    "    setup_optimizers=False,\n",
    "    store_optimizer_states=False,\n",
    ")\n",
    "\n",
    "trainer = nl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=2,\n",
    "    num_nodes=1,\n",
    "    strategy=strategy,\n",
    "    plugins=nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "        params_dtype=torch.bfloat16,\n",
    "        pipeline_dtype=torch.bfloat16,\n",
    "        autocast_enabled=False,\n",
    "        grad_reduce_in_fp32=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "source = {\n",
    "    \"mask\": \"User\",\n",
    "    \"system\": \"\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"User\",\n",
    "            \"value\": \"\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "special_tokens = {\n",
    "                \"system_turn_start\": \"<extra_id_0>\",\n",
    "                \"turn_start\": \"<extra_id_1>\",\n",
    "                \"label_start\": \"<extra_id_2>\",\n",
    "                \"end_of_turn\": \"\\n\",\n",
    "                \"end_of_name\": \"\\n\",\n",
    "            }\n",
    "\n",
    "from nemo.collections.nlp.data.language_modeling.megatron.gpt_sft_chat_dataset import _get_header_conversation_type_mask_role\n",
    "# Apply prompt template to be the same format as training\n",
    "header, conversation, data_type, mask_role = _get_header_conversation_type_mask_role(source, special_tokens)\n",
    "prompts = [conversation]\n",
    "\n",
    "from nemo.collections.llm import api\n",
    "results = api.generate(\n",
    "    path=\"/datasets/soc-20250703225140/nemo_checkpoints/nemotron_49b_super_custom_finetune/2025-07-17_13-54-48/checkpoints/model_name=0--val_loss=0.02-step=2459-consumed_samples=4920.0-last\",\n",
    "    prompts=prompts,\n",
    "    trainer=trainer,\n",
    "    inference_params=CommonInferenceParams(\n",
    "        temperature=1.0,\n",
    "        top_p=0,  # greedy decoding\n",
    "        top_k=1,  # greedy decoding\n",
    "        num_tokens_to_generate=50,\n",
    "    ),\n",
    "    text_only=True,\n",
    ")\n",
    "if torch.distributed.get_rank() == 0:\n",
    "    for i, r in enumerate(results):\n",
    "        print(\"=\" * 50)\n",
    "        print(prompts[i])\n",
    "        print(\"*\" * 50)\n",
    "        if match:\n",
    "            print(match.group(0))\n",
    "        else:\n",
    "            print(r)\n",
    "        print(\"=\" * 50)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00c136fb-63d3-4e2e-b991-710658cd2d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accelerator', 'accumulate_grad_batches', 'barebones', 'benchmark', 'callbacks', 'check_val_every_n_epoch', 'default_root_dir', 'detect_anomaly', 'deterministic', 'devices', 'enable_checkpointing', 'enable_model_summary', 'enable_progress_bar', 'fast_dev_run', 'gradient_clip_algorithm', 'gradient_clip_val', 'inference_mode', 'limit_predict_batches', 'limit_test_batches', 'limit_train_batches', 'limit_val_batches', 'log_every_n_steps', 'logger', 'max_epochs', 'max_steps', 'max_time', 'min_epochs', 'min_steps', 'num_nodes', 'num_sanity_val_steps', 'overfit_batches', 'plugins', 'precision', 'profiler', 'reload_dataloaders_every_n_epochs', 'strategy', 'sync_batchnorm', 'use_distributed_sampler', 'val_check_interval']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from nemo.collections import llm\n",
    "\n",
    "## function\n",
    "# inspect.signature(llm.llama33_nemotron_super_49b)\n",
    "# inspect.signature(llm.LlamaNemotronModel)\n",
    "# inspect.signature(llm.llama33_nemotron_super_49b.finetune_recipe)\n",
    "\n",
    "print(dir(llm.llama33_nemotron_super_49b.finetune_recipe().trainer))\n",
    "print((llm.llama33_nemotron_super_49b.finetune_recipe().trainer.limit_val_batches))\n",
    "# print(dir(llm.llama33_nemotron_super_49b.finetune_recipe().resume.resume_from_directory))\n",
    "# print(dir(llm.llama33_nemotron_super_49b.finetune_recipe().resume)))\n",
    "# print(dir(llm.llama33_nemotron_super_49b.finetune_recipe().log))\n",
    "# print(dir(llm.llama33_nemotron_super_49b))\n",
    "# print(dir(llm.LlamaNemotronModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3deef62f-06c3-4785-a8f9-410632e42ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada63b61-4064-4a46-b041-85284c80e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections import llm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import nemo_run as run\n",
    "from nemo import lightning as nl\n",
    "\n",
    "# checkpoint_path=\"/datasets/soc-20250703225140/nemo_checkpoints/nemotron_49b_super_custom_finetune/2025-07-14_00-47-12/checkpoints/model_name=0--val_loss=0.03-step=499-consumed_samples=1000.0-last/weights/\"\n",
    "\n",
    "# from nemo.collections import llm\n",
    "# llm.import_ckpt(model=llm.LlamaNemotronModel(llm.Llama33NemotronSuper49BConfig()), source=checkpoint_path)\n",
    "\n",
    "# llm.LlamaNemotronModel.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
    "\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "\n",
    "sft_ckpt_path=str(next((d for d in Path(\"/datasets/soc-20250703225140/nemo_checkpoints/nemotron_49b_super_custom_finetune/2025-07-14_00-47-12/checkpoints/\").iterdir() if d.is_dir() and d.name.endswith(\"-last\")), None))\n",
    "print(\"We will load SFT checkpoint from:\", sft_ckpt_path)\n",
    "\n",
    "\n",
    "\n",
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=2,\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=2,\n",
    "        num_nodes=1,\n",
    "        strategy=strategy,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "prompts = [\n",
    "    \"How many r's are in the word 'strawberry'?\",\n",
    "    \"Which number is bigger? 10.119 or 10.19?\",\n",
    "]\n",
    "\n",
    "def configure_inference():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(sft_ckpt_path),\n",
    "        trainer=trainer(),\n",
    "        prompts=prompts,\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=8192, top_p=0.95),\n",
    "        output_path=\"sft_prediction.jsonl\",\n",
    "    )\n",
    "\n",
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 2) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "        \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\",\n",
    "        \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_inference(), executor=local_executor_torchrun())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a043c6ce-5132-4106-985b-6fd29a49bba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He's\n"
     ]
    }
   ],
   "source": [
    "print(\"He's\".replace(\"'\", \"\\\\'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c78c5ae6-4ead-429b-bb72-7f4380a560ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clinical Pharmacogenetics Implementation Consortium (CPIC) Guidelines for Ivacaftor Therapy in the Context of CFTR Genotype (March 2014).pdf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data_process.guideline_names_extraction\n",
    "# get_guideline_name()\n",
    "import os\n",
    "cpic_guideline_name_list = os.listdir(\"./data_process/Guidelines\")\n",
    "cpic_guideline_name_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879d6a94-4343-4ac6-8dbb-5619394dfc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 轉換完成：4391 筆（跳過 0 筆）\n",
      "  → training.jsonl    3073 行  （驗證通過）\n",
      "  → validation.jsonl  658 行  （驗證通過）\n",
      "  → test.jsonl        660 行  （驗證通過）\n",
      "\n",
      "🎉  全部完成！可直接交給 NeMo 進行 fine-tune。\n"
     ]
    }
   ],
   "source": [
    "## 最重要程式碼\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "convert_nemo_dataset.py\n",
    "\n",
    "將 dataset.json 轉成 NeMo Chat-SFT 所需的\n",
    "training.jsonl / validation.jsonl / test.jsonl（三檔）。\n",
    "\n",
    "特點\n",
    "1. 徹底清理所有隱藏換行 ( \\n, \\r, U+2028, U+2029 )\n",
    "2. 對 dict / list value 先做 json.dumps，保證單行\n",
    "3. 每筆寫入前以 json.dumps() 驗證；含換行就強制丟棄\n",
    "4. 寫檔完成後再次深度掃描，若仍有殘留換行即拋錯\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1.  基本設定\n",
    "# --------------------------------------------------------------------\n",
    "SRC_JSON         = \"/datasets/soc-20250703225140/dataset/dataset.json\"\n",
    "OUT_DIR          = \"/datasets/soc-20250703225140/dataset_split\"\n",
    "SPLIT_RATIOS     = (0.70, 0.15, 0.15)  # train / val / test\n",
    "RANDOM_SEED      = 42\n",
    "NEWLINES         = (\"\\n\", \"\\r\", \"\\u2028\", \"\\u2029\")       # 所有可能殘留的換行符\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2.  工具函式\n",
    "# --------------------------------------------------------------------\n",
    "def sanitize(text: Any) -> str:\n",
    "    \"\"\"將任意物件轉為『單行、安全字串』\"\"\"\n",
    "    s = str(text)\n",
    "    # 換行符 → 空白\n",
    "    for ch in NEWLINES:\n",
    "        s = s.replace(ch, \" \")\n",
    "    # 其他空白壓縮\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # 跳脫雙引號\n",
    "    return s.replace('\"', r'\\\"')\n",
    "\n",
    "\n",
    "def format_answer(ans: Any) -> str:\n",
    "    \"\"\"\n",
    "    把 answer 轉成 <QUERYCALL>[ ... ]</QUERYCALL>\n",
    "      - 若為 dict：逐 key/value 處理\n",
    "      - 其他型別：視為拒答文字，直接 sanitize\n",
    "    \"\"\"\n",
    "    if not isinstance(ans, dict):\n",
    "        return sanitize(ans)\n",
    "\n",
    "    parts = []\n",
    "    for k, v in ans.items():\n",
    "        if isinstance(v, (dict, list)):\n",
    "            v = json.dumps(v, separators=(\",\", \":\"))  # 巢狀資料 → 壓成單行 JSON\n",
    "        parts.append(f'{sanitize(k)}=\"{sanitize(v)}\"')\n",
    "    return f\"<QUERYCALL>[{', '.join(parts)}]</QUERYCALL>\"\n",
    "\n",
    "\n",
    "def has_hidden_newline(s: str) -> bool:\n",
    "    \"\"\"判斷字串是否仍含 NEWLINES 中的任一字元\"\"\"\n",
    "    return any(ch in s for ch in NEWLINES)\n",
    "\n",
    "\n",
    "def deep_scan_jsonl(path: Path) -> List[int]:\n",
    "    \"\"\"\n",
    "    掃描 .jsonl，回傳『仍含隱藏換行符』的行號清單\n",
    "    備註：使用 newline=\"\" 讀檔，才不會被 Python 自動轉譯\n",
    "    \"\"\"\n",
    "    bad_lines = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        for ln, raw in enumerate(f, 1):\n",
    "            # 去除真正的行尾\n",
    "            if raw.endswith(\"\\r\\n\"):\n",
    "                core = raw[:-2]\n",
    "            elif raw.endswith(\"\\n\"):\n",
    "                core = raw[:-1]\n",
    "            else:\n",
    "                core = raw\n",
    "            if has_hidden_newline(core):\n",
    "                bad_lines.append(ln)\n",
    "    return bad_lines\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3.  主要流程\n",
    "# --------------------------------------------------------------------\n",
    "def convert_dataset(\n",
    "    src_json: str,\n",
    "    out_dir: str,\n",
    "    split: tuple = SPLIT_RATIOS,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> None:\n",
    "    random.seed(seed)\n",
    "\n",
    "    # 讀入原始 JSON\n",
    "    with open(src_json, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"輸入檔案必須是 JSON list。\")\n",
    "\n",
    "    processed: List[Dict] = []\n",
    "    skipped = 0\n",
    "\n",
    "    for idx, item in enumerate(data, 1):\n",
    "        if not (isinstance(item, dict) and \"question\" in item and \"answer\" in item):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        record = {\n",
    "            \"system\": \"\",\n",
    "            \"mask\": \"User\",\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"User\",      \"value\": sanitize(item[\"question\"])},\n",
    "                {\"from\": \"Assistant\", \"value\": format_answer(item[\"answer\"])},\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # 寫入前再次驗證\n",
    "        line = json.dumps(record, ensure_ascii=False)\n",
    "        if has_hidden_newline(line):\n",
    "            print(f\"⚠️  跳過第 {idx} 筆：仍含隱藏換行符\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        processed.append(record)\n",
    "\n",
    "    print(f\"✔ 轉換完成：{len(processed)} 筆（跳過 {skipped} 筆）\")\n",
    "\n",
    "    # 隨機打亂\n",
    "    random.shuffle(processed)\n",
    "\n",
    "    # 切分\n",
    "    n        = len(processed)\n",
    "    n_train  = int(n * split[0])\n",
    "    n_val    = int(n * split[1])\n",
    "\n",
    "    splits = {\n",
    "        \"training.jsonl\":   processed[:n_train],\n",
    "        \"validation.jsonl\": processed[n_train:n_train + n_val],\n",
    "        \"test.jsonl\":       processed[n_train + n_val:],\n",
    "    }\n",
    "\n",
    "    # 輸出\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    for fname, recs in splits.items():\n",
    "        path = Path(out_dir) / fname\n",
    "        with path.open(\"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "            for r in recs:\n",
    "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # 寫完後做深度掃描\n",
    "        bad = deep_scan_jsonl(path)\n",
    "        if bad:\n",
    "            raise RuntimeError(f\"{fname} 存在隱藏換行符，壞行號：{bad[:10]}\")\n",
    "        print(f\"  → {fname:<17} {len(recs)} 行  （驗證通過）\")\n",
    "\n",
    "    print(\"\\n🎉  全部完成！可直接交給 NeMo 進行 fine-tune。\")\n",
    "\n",
    "\n",
    "\n",
    "convert_dataset(SRC_JSON, OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020c3971-dcc5-4925-8a39-38ce01883a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/deep_ep-1.0.0+a84a248-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: lightning-utilities in /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg (0.14.0)\n",
      "Collecting lightning-utilities\n",
      "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.12/dist-packages (from lightning-utilities) (23.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities) (78.1.1)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities) (4.12.2)\n",
      "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: lightning-utilities\n",
      "  Attempting uninstall: lightning-utilities\n",
      "    Found existing installation: lightning-utilities 0.14.0\n",
      "    Uninstalling lightning-utilities-0.14.0:\n",
      "      Successfully uninstalled lightning-utilities-0.14.0\n",
      "Successfully installed lightning-utilities-0.14.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade lightning-utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad3ca60-739d-47b3-bc53-049f6e8d0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME']='/datasets/soc-20250703225140/'\n",
    "os.environ['HF_HUB_CACHE']='/datasets/soc-20250703225140/'\n",
    "os.environ['TRANSFORMERS_CACHE']='/datasets/soc-20250703225140/'\n",
    "os.environ['NEMO_HOME']='/datasets/soc-20250703225140/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ecbc84-fdc4-4728-b61d-321a79a3b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama-3_3-Nemotron-Super-49B-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb1f8d-63c6-4b6e-b88f-162af67d6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27c1cf86-4f2b-48f8-8ebb-e54c8ba3d2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-10 23:17:15 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> $</span><span style=\"color: #008000; text-decoration-color: #008000\">NEMO_MODELS_CACHE</span><span style=\"color: #008000; text-decoration-color: #008000\">=/datasets/soc-20250703225140/models </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m $\u001b[0m\u001b[32mNEMO_MODELS_CACHE\u001b[0m\u001b[32m=\u001b[0m\u001b[32m/datasets/soc-20250703225140/\u001b[0m\u001b[32mmodels\u001b[0m\u001b[32m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Imported Checkpoint</span>\n",
       "├── <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">context/</span>\n",
       "│   ├── <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">artifacts/</span>\n",
       "│   │   └── <span style=\"color: #808000; text-decoration-color: #808000\">generation_config.json</span>\n",
       "│   ├── <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo_tokenizer/</span>\n",
       "│   │   ├── <span style=\"color: #808000; text-decoration-color: #808000\">special_tokens_map.json</span>\n",
       "│   │   ├── <span style=\"color: #808000; text-decoration-color: #808000\">tokenizer.json</span>\n",
       "│   │   └── <span style=\"color: #808000; text-decoration-color: #808000\">tokenizer_config.json</span>\n",
       "│   ├── <span style=\"color: #808000; text-decoration-color: #808000\">io.json</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">model.yaml</span>\n",
       "└── <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">weights/</span>\n",
       "    ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">.metadata</span>\n",
       "    ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">__0_0.distcp</span>\n",
       "    ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">__0_1.distcp</span>\n",
       "    ├── <span style=\"color: #800080; text-decoration-color: #800080\">common.pt</span>\n",
       "    └── <span style=\"color: #808000; text-decoration-color: #808000\">metadata.json</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mImported Checkpoint\u001b[0m\n",
       "├── \u001b[1;36mcontext/\u001b[0m\n",
       "│   ├── \u001b[1;36martifacts/\u001b[0m\n",
       "│   │   └── \u001b[33mgeneration_config.json\u001b[0m\n",
       "│   ├── \u001b[1;36mnemo_tokenizer/\u001b[0m\n",
       "│   │   ├── \u001b[33mspecial_tokens_map.json\u001b[0m\n",
       "│   │   ├── \u001b[33mtokenizer.json\u001b[0m\n",
       "│   │   └── \u001b[33mtokenizer_config.json\u001b[0m\n",
       "│   ├── \u001b[33mio.json\u001b[0m\n",
       "│   └── \u001b[37mmodel.yaml\u001b[0m\n",
       "└── \u001b[1;36mweights/\u001b[0m\n",
       "    ├── \u001b[37m.metadata\u001b[0m\n",
       "    ├── \u001b[37m__0_0.distcp\u001b[0m\n",
       "    ├── \u001b[37m__0_1.distcp\u001b[0m\n",
       "    ├── \u001b[35mcommon.pt\u001b[0m\n",
       "    └── \u001b[33mmetadata.json\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/datasets/soc-20250703225140/models/nvidia/Llama-3_3-Nemotron-Super-49B-v1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nemo.collections import llm\n",
    "llm.import_ckpt(model=llm.LlamaNemotronModel(llm.Llama33NemotronSuper49BConfig()), source='hf://nvidia/Llama-3_3-Nemotron-Super-49B-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51f28a-e3ce-4a13-8b33-c730da5f60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(llm.Llama33NemotronSuper49BConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b5e87d7-47ac-44ba-b1c5-d96cd6bb881c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (dir: Optional[str] = None, name: str = 'default', num_nodes: int = 1, num_gpus_per_node: int = 8, peft_scheme: Optional[str] = 'lora', seq_length: Optional[int] = None, packed_sequence: Optional[bool] = None, performance_mode: bool = False) -> nemo_run.config.Partial>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "from nemo.collections import llm\n",
    "\n",
    "inspect.signature(llm.llama33_nemotron_super_49b.finetune_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00262116-12a1-4b3c-b16b-bc4df4b8583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections import llm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "checkpoint_path=\"/datasets/soc-20250703225140/models--nvidia--Llama-3_3-Nemotron-Super-49B-v1\"\n",
    "\n",
    "\n",
    "recipe = llm.llama33_nemotron_super_49b.finetune_recipe(\n",
    "    name=\"llama33_nemotron_super_49b_finetuning\",\n",
    "    dir=checkpoint_path,\n",
    "    num_nodes=2,\n",
    "    num_gpus_per_node=1,\n",
    "    \n",
    "    peft_scheme='lora',  # 'lora', 'none'\n",
    "    packed_sequence=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade56a72-d00b-4af8-80b3-9c6369c092b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(recipe.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2941d93b-dc23-49a9-bef5-159685af4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast # 導入 ast 模組\n",
    "\n",
    "def read_qa_pairs_from_file() -> list:\n",
    "    \"\"\"\n",
    "    從指定路徑的檔案中讀取問題-答案配對。\n",
    "    假設檔案內容是一個包含多個字典的 JSON 陣列，每個字典有 \"question\" 和 \"answer\" 鍵。\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 資料檔案的路徑。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含問題-答案字典的列表。\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: 如果檔案不存在。\n",
    "        ValueError: 如果檔案內容不是預期的列表或字典格式。\n",
    "    \"\"\"\n",
    "    # 移除這裡的虛擬資料檔創建邏輯，讓函式專注於讀取\n",
    "    # 這兩個路徑應該作為參數傳入 read_qa_pairs_from_file 或在外部定義\n",
    "    negative_sample_path = \"/datasets/soc-20250703225140/dataset/negative_sample.txt\"\n",
    "    creative_sample_path = \"/datasets/soc-20250703225140/dataset/creative_dataset.txt\"\n",
    "    \n",
    "    negative_list = []\n",
    "    creative_dataset = None\n",
    "    \n",
    "    # 從 negative_sample.txt 讀取，使用 ast.literal_eval\n",
    "    with open(negative_sample_path, 'r', encoding='utf-8') as f:\n",
    "        negative_list = ast.literal_eval(f.read())\n",
    "    \n",
    "    # 從 creative_dataset.txt 讀取，使用 json.load\n",
    "    with open(creative_sample_path, \"r\", encoding='utf-8') as f:\n",
    "        creative_dataset = json.load(f)\n",
    "\n",
    "    creative_list = []\n",
    "    for creative_data_item in creative_dataset: # 修改變數名以避免混淆\n",
    "        # 假設 creative_data_item 是 {'key': ['qa1', 'qa2']} 的形式\n",
    "        for key, value_list in creative_data_item.items():\n",
    "            creative_list.extend(value_list) # 使用 extend 添加列表中的所有元素\n",
    "        \n",
    "    qa_pairs = negative_list + creative_list\n",
    "\n",
    "    if not isinstance(qa_pairs, list) or not all(isinstance(item, dict) and \"question\" in item and \"answer\" in item for item in qa_pairs):\n",
    "        raise ValueError(\"檔案內容不是有效的 Q&A 字典列表格式。請確認每個項目都是包含 'question' 和 'answer' 鍵的字典。\")\n",
    "\n",
    "    return qa_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64387024-416f-4f87-8926-7487c5e0c604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the CPIC-recommended dose for metformin in a patient with the CYP2D6 *4/*4 genotype?',\n",
       " 'answer': 'No CPIC guideline information available.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57759a95-c7a6-4121-9b41-d353135859d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creative_data[0]\n",
    "# creative_list = []\n",
    "# for creative_data in creative_dataset:\n",
    "#     for _, value in creative_data.items():\n",
    "#         creative_list += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6a0f7ed-f35a-43f7-b1d7-f5bb910a383c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the recommended ivacaftor dose for a patient with the CFTR G551D mutation?',\n",
       " 'answer': {'Drug Name': 'ivacaftor',\n",
       "  'Gene Name': 'CFTR',\n",
       "  'CPIC Guideline Name': 'Clinical Pharmacogenetics Implementation Consortium (CPIC) Guidelines for Ivacaftor Therapy in the Context of CFTR Genotype (March 2014).pdf',\n",
       "  'Content to Search': 'Recommended ivacaftor dosage for patients with the CFTR G551D genotype.'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creative_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9fdf572-f3c8-4391-a594-477571c630d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_dataset = creative_list + negative_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e956ee-aacb-40da-b8dc-b21b4508cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer # 假設你會用 Hugging Face 的分詞器\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs: list, tokenizer: AutoTokenizer, max_length: int = 512):\n",
    "        \"\"\"\n",
    "        初始化自定義的問答資料集。\n",
    "\n",
    "        Args:\n",
    "            qa_pairs (list): 從檔案讀取的問題-答案字典列表。\n",
    "            tokenizer (AutoTokenizer): 用於處理文本的 Hugging Face 分詞器。\n",
    "            max_length (int): 序列的最大長度，用於分詞器的截斷。\n",
    "        \"\"\"\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回資料集中的問題-答案對的總數\n",
    "        return len(self.qa_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 根據索引獲取一個單個樣本\n",
    "        item = self.qa_pairs[idx]\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "\n",
    "        # 這裡返回原始字串，分詞和填充的步驟將在 `collate_fn` 中處理。\n",
    "        return {\"question\": question, \"answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21c606d-5892-42bd-b201-f0df9f5d607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "def custom_collate_fn(batch: list, tokenizer: AutoTokenizer, max_length: int):\n",
    "    \"\"\"\n",
    "    自定義的批次化函式。將多個 Q&A 樣本處理成模型可接受的單個批次。\n",
    "\n",
    "    Args:\n",
    "        batch (list): 包含多個 `__getitem__` 返回的字典（例如 [{\"question\": \"...\", \"answer\": \"...\"}, ...]）。\n",
    "        tokenizer (AutoTokenizer): Hugging Face 分詞器。\n",
    "        max_length (int): 序列的最大長度。\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含 'input_ids', 'attention_mask' 和 'labels' 的字典，適合模型訓練。\n",
    "    \"\"\"\n",
    "    questions = [item[\"question\"] for item in batch]\n",
    "    answers = [item[\"answer\"] for item in batch]\n",
    "\n",
    "    # 將問題和答案組合成一個用於因果語言建模的訓練格式。\n",
    "    # 這種格式常見於微調模型以生成答案。\n",
    "    # 通常的格式為：\"問題: [你的問題]\\n答案: [你的答案]<eos_token>\"\n",
    "    texts_to_tokenize = [f\"問題: {q}\\n答案: {a}{tokenizer.eos_token}\" for q, a in zip(questions, answers)]\n",
    "\n",
    "    # 對整個批次的文本進行分詞和填充\n",
    "    tokenized_batch = tokenizer(\n",
    "        texts_to_tokenize,\n",
    "        max_length=max_length, # 將所有序列填充或截斷到這個長度\n",
    "        truncation=True,       # 如果序列超過 max_length，則截斷\n",
    "        padding=\"max_length\",  # 填充到 max_length\n",
    "        return_tensors=\"pt\"    # 返回 PyTorch 張量\n",
    "    )\n",
    "\n",
    "    # 對於因果語言模型 (Causal Language Model)，通常 `input_ids` 也用作 `labels`。\n",
    "    # 模型會在內部將 `labels` 偏移一個位置來進行下一個詞的預測。\n",
    "    # 我們複製一份，因為 labels 可能會被修改 (例如 -100 填充)。\n",
    "    labels = tokenized_batch[\"input_ids\"].clone()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_batch[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_batch[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23288e79-630d-487f-b84d-bced9646a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_function_that_configures_your_custom_dataset(gbs: int, mbs: int, seq_length: int, tokenizer: AutoTokenizer) -> DataLoader:\n",
    "    \"\"\"\n",
    "    配置並返回一個用於 NeMo Recipe 的客製化 DataLoader。\n",
    "\n",
    "    Args:\n",
    "        gbs (int): Global Batch Size (總批次大小)。\n",
    "        mbs (int): Micro Batch Size (每個 GPU 的批次大小)。這將作為 DataLoader 的 batch_size。\n",
    "        seq_length (int): 序列最大長度。\n",
    "        tokenizer (AutoTokenizer): Hugging Face 分詞器。\n",
    "        qa_file_path (str): 你的 Q&A 數據檔案的路徑。\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: 配置好的數據載入器。\n",
    "    \"\"\"\n",
    "    print(f\"\\nConfiguring DataLoader with: Global Batch Size={gbs}, Micro Batch Size={mbs}, Sequence Length={seq_length}\")\n",
    "    \n",
    "    # 讀取你的 Q&A 數據\n",
    "    qa_pairs = read_qa_pairs_from_file()\n",
    "    \n",
    "    # 實例化你的客製化數據集\n",
    "    custom_dataset_instance = QADataset(qa_pairs=qa_pairs, tokenizer=tokenizer, max_length=seq_length)\n",
    "    print(f\"Custom dataset created with {len(custom_dataset_instance)} samples.\")\n",
    "    \n",
    "    # 創建 DataLoader 實例\n",
    "    # DataLoader 的 batch_size 參數直接對應每個 GPU 的 micro batch size (mbs)\n",
    "    custom_dataloader = DataLoader(\n",
    "        custom_dataset_instance,\n",
    "        batch_size=mbs,\n",
    "        shuffle=True, # 訓練時通常需要打亂數據\n",
    "        num_workers=0, # 在 Jupyter/Colab 環境中，0 通常是安全的選擇\n",
    "        pin_memory=True, # 啟用 pin_memory 可以加速數據傳輸到 GPU\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer=tokenizer, max_length=seq_length)\n",
    "    )\n",
    "    print(f\"DataLoader created with {len(custom_dataloader)} batches (each of size {mbs}).\")\n",
    "    return custom_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b1f75a-1791-43b9-84ab-eab7ec680f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分詞器的 pad_token 已設定為 eos_token: 128009\n",
      "\n",
      "Configuring DataLoader with: Global Batch Size=8, Micro Batch Size=4, Sequence Length=2048\n",
      "Custom dataset created with 4391 samples.\n",
      "DataLoader created with 1098 batches (each of size 4).\n"
     ]
    }
   ],
   "source": [
    "# --- 範例使用方式 (模擬 NeMo Recipe 和 run.run) ---\n",
    "# if __name__ == \"__main__\":\n",
    "# --- 必要設定與檔案定義 ---\n",
    "\n",
    "# 設定快取目錄 (確保有寫入權限)\n",
    "\n",
    "cache_dir = '/datasets/soc-20250703225140/'\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_HUB_CACHE'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['NEMO_HOME']=cache_dir\n",
    "\"\"\"\n",
    "export HF_HOME=/datasets/soc-20250703225140/\n",
    "export HF_HUB_CACHE=/datasets/soc-20250703225140/\n",
    "export TRANSFORMERS_CACHE=/datasets/soc-20250703225140/\n",
    "export NEMO_HOME=/datasets/soc-20250703225140/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# --- 載入分詞器 ---\n",
    "model_name=\"nvidia/Llama-3_3-Nemotron-Super-49B-v1\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"分詞器的 pad_token 已設定為 eos_token: {tokenizer.pad_token_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"錯誤：無法載入分詞器 '{tokenizer_model_id}'。{e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 模擬 NeMo Recipe 參數 ---\n",
    "# 這些參數通常從 NeMo Recipe 或其模型配置中獲取\n",
    "# 確保這些值與你的實際訓練配置相符\n",
    "gbs_param = 8   # Global Batch Size\n",
    "mbs_param = 4   # Micro Batch Size per GPU\n",
    "\n",
    "\n",
    "# --- 呼叫 a_function_that_configures_your_custom_dataset 來獲取 DataLoader ---\n",
    "dataloader = a_function_that_configures_your_custom_dataset(\n",
    "    gbs=gbs_param,\n",
    "    mbs=mbs_param,\n",
    "    seq_length=recipe.model.config.seq_length,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9510c010-1dab-4461-89c2-db4362b1c4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分詞器的 pad_token 已設定為 eos_token: 128009\n",
      "\n",
      "Configuring DataLoader with: Global Batch Size=8, Micro Batch Size=4, Sequence Length=2048\n",
      "Custom dataset created with 4391 samples.\n",
      "DataLoader created with 1098 batches (each of size 4).\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/datasets/soc-20250703225140/'\n",
    "# os.makedirs(my_hf_cache_dir, exist_ok=True)\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_HUB_CACHE'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['NEMO_HOME']=cache_dir\n",
    "\"\"\"\n",
    "export HF_HOME=/datasets/soc-20250703225140/\n",
    "export HF_HUB_CACHE=/datasets/soc-20250703225140/\n",
    "export TRANSFORMERS_CACHE=/datasets/soc-20250703225140/\n",
    "export NEMO_HOME=/datasets/soc-20250703225140/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# --- 載入分詞器 ---\n",
    "model_name=\"nvidia/Llama-3_3-Nemotron-Super-49B-v1\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"分詞器的 pad_token 已設定為 eos_token: {tokenizer.pad_token_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"錯誤：無法載入分詞器 '{tokenizer_model_id}'。{e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 模擬 NeMo Recipe 參數 ---\n",
    "# 這些參數通常從 NeMo Recipe 或其模型配置中獲取\n",
    "# 確保這些值與你的實際訓練配置相符\n",
    "gbs_param = 8   # Global Batch Size\n",
    "mbs_param = 4   # Micro Batch Size per GPU\n",
    "\n",
    "\n",
    "# --- 呼叫 a_function_that_configures_your_custom_dataset 來獲取 DataLoader ---\n",
    "dataloader = a_function_that_configures_your_custom_dataset(\n",
    "    gbs=gbs_param,\n",
    "    mbs=mbs_param,\n",
    "    seq_length=recipe.model.config.seq_length,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d73d686-a696-4085-a52a-3eda94e0f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring DataLoader with: Global Batch Size=8, Micro Batch Size=4, Sequence Length=2048\n",
      "Custom dataset created with 4391 samples.\n",
      "DataLoader created with 1098 batches (each of size 4).\n"
     ]
    }
   ],
   "source": [
    "dataloader = a_function_that_configures_your_custom_dataset(\n",
    "    gbs=gbs_param,\n",
    "    mbs=mbs_param,\n",
    "    tokenizer=tokenizer,\n",
    "    seq_length=recipe.model.config.seq_length,\n",
    ")\n",
    "recipe.data = dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a33168-f97c-4530-92c5-4ce66e261dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1751990357</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─── \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1751990357\u001b[0m\u001b[92m ───\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnserializableValueError",
     "evalue": "Unserializable value <torch.utils.data.dataloader.DataLoader object at 0x7f18c47ebce0> of type <class 'torch.utils.data.dataloader.DataLoader'>. Error occurred at path '<root>.data'.\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnserializableValueError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m recipe.trainer.strategy.tensor_model_parallel_size = \u001b[32m2\u001b[39m\n\u001b[32m      4\u001b[39m recipe.trainer.strategy.pipeline_model_parallel_size = \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLocalExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/run/api.py:80\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(fn_or_script, executor, plugins, name, dryrun, direct, detach, tail_logs, log_level)\u001b[39m\n\u001b[32m     78\u001b[39m name = name \u001b[38;5;129;01mor\u001b[39;00m default_name\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Experiment(title=name, executor=executor, log_level=log_level) \u001b[38;5;28;01mas\u001b[39;00m exp:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn_or_script\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_logs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtail_logs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dryrun:\n\u001b[32m     82\u001b[39m         exp.dryrun()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/run/experiment.py:542\u001b[39m, in \u001b[36mExperiment.add\u001b[39m\u001b[34m(self, task, executor, name, plugins, tail_logs, dependencies)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m executor \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor, Executor)\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     job_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_single_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtail_logs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtail_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m name, \u001b[33m\"\u001b[39m\u001b[33mname is required for task group.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/run/experiment.py:421\u001b[39m, in \u001b[36mExperiment._add_single_job\u001b[39m\u001b[34m(self, task, executor, name, plugins, tail_logs, dependencies)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    419\u001b[39m     task_id = name\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m executor = executor.clone()\n\u001b[32m    424\u001b[39m executor.assign(\n\u001b[32m    425\u001b[39m     \u001b[38;5;28mself\u001b[39m._id,\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m._exp_dir,\n\u001b[32m    427\u001b[39m     task_id=task_id,\n\u001b[32m    428\u001b[39m     task_dir=name \u001b[38;5;28;01mif\u001b[39;00m reuse_job_dir \u001b[38;5;28;01melse\u001b[39;00m task_id,\n\u001b[32m    429\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/run/experiment.py:500\u001b[39m, in \u001b[36mExperiment._validate_task\u001b[39m\u001b[34m(self, task_info, task)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, Partial):\n\u001b[32m    499\u001b[39m     serializer = ZlibJSONSerializer()\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     serialized = \u001b[43mserializer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m     deserialized = serializer.deserialize(serialized)\n\u001b[32m    502\u001b[39m     diff = diffing.build_diff(deserialized, task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo-Run/nemo_run/core/serialization/zlib_json.py:33\u001b[39m, in \u001b[36mZlibJSONSerializer.serialize\u001b[39m\u001b[34m(self, cfg, pyref_policy)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mserialize\u001b[39m(\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     29\u001b[39m     cfg: config.Buildable,\n\u001b[32m     30\u001b[39m     pyref_policy: Optional[serialization.PyrefPolicy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     31\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m base64.urlsafe_b64encode(\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         zlib.compress(\u001b[43mserialization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyref_policy\u001b[49m\u001b[43m)\u001b[49m.encode())\n\u001b[32m     34\u001b[39m     ).decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/fiddle/_src/experimental/serialization.py:826\u001b[39m, in \u001b[36mdump_json\u001b[39m\u001b[34m(value, pyref_policy, indent)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump_json\u001b[39m(\n\u001b[32m    803\u001b[39m     value: Any,\n\u001b[32m    804\u001b[39m     pyref_policy: Optional[PyrefPolicy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    805\u001b[39m     indent: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    806\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    807\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns the JSON serialization of `value`.\u001b[39;00m\n\u001b[32m    808\u001b[39m \n\u001b[32m    809\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m \u001b[33;03m      disallowed by `pyref_policy`.\u001b[39;00m\n\u001b[32m    825\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m json.dumps(\u001b[43mSerialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyref_policy\u001b[49m\u001b[43m)\u001b[49m.result, indent=indent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/fiddle/_src/experimental/serialization.py:527\u001b[39m, in \u001b[36mSerialization.__init__\u001b[39m\u001b[34m(self, value, pyref_policy)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28mself\u001b[39m._pyref_policy = pyref_policy \u001b[38;5;129;01mor\u001b[39;00m DefaultPyrefPolicy()\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# The result of the serialization.\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[38;5;28mself\u001b[39m._result = {\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     _ROOT_KEY: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    528\u001b[39m     _OBJECTS_KEY: \u001b[38;5;28mself\u001b[39m._objects,\n\u001b[32m    529\u001b[39m     _REFCOUNTS_KEY: \u001b[38;5;28mself\u001b[39m._refcounts,\n\u001b[32m    530\u001b[39m     _VERSION_KEY: _VERSION,\n\u001b[32m    531\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo/nemo/lightning/io/fdl_torch.py:134\u001b[39m, in \u001b[36menable.<locals>._modified_serialize\u001b[39m\u001b[34m(self, value, current_path, all_paths)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, types.BuiltinFunctionType):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pyref(value, current_path)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_original_serialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/fiddle/_src/experimental/serialization.py:662\u001b[39m, in \u001b[36mSerialization._serialize\u001b[39m\u001b[34m(self, value, current_path, all_paths)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path_element, child_value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(path_elements, values):\n\u001b[32m    659\u001b[39m   child_paths = (\n\u001b[32m    660\u001b[39m       daglish.add_path_element(all_paths, path_element)\n\u001b[32m    661\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m all_paths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m   serialized_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m      \u001b[49m\u001b[43mchild_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_element\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchild_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m   \u001b[38;5;66;03m# The serialized item is a two-element list with a string representation\u001b[39;00m\n\u001b[32m    665\u001b[39m   \u001b[38;5;66;03m# of the path element available for debugging/visualization purposes.\u001b[39;00m\n\u001b[32m    666\u001b[39m   serialized_item = (\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_element\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m, serialized_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo/nemo/lightning/io/fdl_torch.py:134\u001b[39m, in \u001b[36menable.<locals>._modified_serialize\u001b[39m\u001b[34m(self, value, current_path, all_paths)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, types.BuiltinFunctionType):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pyref(value, current_path)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_original_serialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/fiddle/_src/experimental/serialization.py:651\u001b[39m, in \u001b[36mSerialization._serialize\u001b[39m\u001b[34m(self, value, current_path, all_paths)\u001b[39m\n\u001b[32m    648\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    649\u001b[39m     msg = (\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnserializable value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Error \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    650\u001b[39m            \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33moccurred at path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_str(current_path)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnserializableValueError(msg)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# We have a traverser; serialize value's flattened elements.\u001b[39;00m\n\u001b[32m    653\u001b[39m   values, metadata = traverser.flatten(value)\n",
      "\u001b[31mUnserializableValueError\u001b[39m: Unserializable value <torch.utils.data.dataloader.DataLoader object at 0x7f18c47ebce0> of type <class 'torch.utils.data.dataloader.DataLoader'>. Error occurred at path '<root>.data'.\")"
     ]
    }
   ],
   "source": [
    "import nemo_run as run\n",
    "\n",
    "recipe.trainer.strategy.tensor_model_parallel_size = 2\n",
    "recipe.trainer.strategy.pipeline_model_parallel_size = 1\n",
    "\n",
    "run.run(recipe, executor=run.LocalExecutor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bbf2a-7487-43e1-b5e0-c883bc456b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nemo.collections import llm\n",
    "import nemo_run as run\n",
    "from nemo.collections.common.data.datamodule import DataModule as NeMoDataModule\n",
    "\n",
    "# --- 0. Setup: Define necessary paths and ensure directories exist ---\n",
    "my_hf_cache_dir = \"/datasets/soc-20250703225140/\" # Your specified cache dir\n",
    "os.makedirs(my_hf_cache_dir, exist_ok=True)\n",
    "os.environ['HF_HOME'] = my_hf_cache_dir\n",
    "os.environ['HF_HUB_CACHE'] = my_hf_cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = my_hf_cache_dir\n",
    "os.environ['NEMO_HOME'] = my_hf_cache_dir\n",
    "\n",
    "checkpoint_dir = \"/datasets/soc-20250703225140/nemo_checkpoints\" # Example, ensure this exists\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "qa_data_file = \"/datasets/soc-20250703225140/dataset/negative_sample.txt\" # Your data file path\n",
    "\n",
    "# --- 1. Custom Dataset Class ---\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs: list, tokenizer: AutoTokenizer, max_length: int = 512):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.qa_pairs[idx]\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        return {\"question\": question, \"answer\": answer}\n",
    "\n",
    "# --- 2. Custom Collate Function ---\n",
    "def collate_fn(batch: list, tokenizer: AutoTokenizer, max_length: int):\n",
    "    questions = [item[\"question\"] for item in batch]\n",
    "    answers = [item[\"answer\"] for item in batch]\n",
    "    texts_to_tokenize = [f\"問題: {q}\\n答案: {a}{tokenizer.eos_token}\" for q, a in zip(questions, answers)]\n",
    "    tokenized_batch = tokenizer(\n",
    "        texts_to_tokenize,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenized_batch[\"input_ids\"].clone()\n",
    "    return {\n",
    "        \"input_ids\": tokenized_batch[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_batch[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# --- 3. Read QA Pairs From File (Adjusted for your specific files) ---\n",
    "def read_qa_pairs_from_file() -> list: # Removed file_path arg as paths are hardcoded\n",
    "    negative_sample_path = \"/datasets/soc-20250703225140/dataset/negative_sample.txt\"\n",
    "    creative_sample_path = \"/datasets/soc-20250703225140/dataset/creative_dataset.txt\" # Assuming this is JSON\n",
    "\n",
    "    negative_list = []\n",
    "    creative_dataset = None\n",
    "\n",
    "    # Read negative_sample.txt (assumed to be Python list string)\n",
    "    if os.path.exists(negative_sample_path):\n",
    "        with open(negative_sample_path, 'r', encoding='utf-8') as f:\n",
    "            negative_list = ast.literal_eval(f.read())\n",
    "    else:\n",
    "        print(f\"Warning: {negative_sample_path} not found. Skipping negative samples.\")\n",
    "\n",
    "    # Read creative_dataset.txt (assumed to be JSON)\n",
    "    if os.path.exists(creative_sample_path):\n",
    "        with open(creative_sample_path, \"r\", encoding='utf-8') as f:\n",
    "            creative_dataset = json.load(f)\n",
    "    else:\n",
    "        print(f\"Warning: {creative_sample_path} not found. Skipping creative dataset.\")\n",
    "        creative_dataset = []\n",
    "\n",
    "    creative_list = []\n",
    "    if creative_dataset:\n",
    "        for creative_data_item in creative_dataset:\n",
    "            for _, value_list in creative_data_item.items():\n",
    "                creative_list.extend(value_list)\n",
    "        \n",
    "    qa_pairs = negative_list + creative_list\n",
    "\n",
    "    if not isinstance(qa_pairs, list) or not all(isinstance(item, dict) and \"question\" in item and \"answer\" in item for item in qa_pairs):\n",
    "        raise ValueError(\"Combined data is not a valid list of Q&A dictionaries.\")\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "# --- 4. Custom DataModule (as previously defined) ---\n",
    "class CustomQAPairDataModule(NeMoDataModule):\n",
    "    def __init__(self,\n",
    "                 qa_file_path: str, # Now takes the main combined QA file path\n",
    "                 tokenizer: AutoTokenizer, # Still pass tokenizer instance\n",
    "                 max_length: int,\n",
    "                 gbs: int,\n",
    "                 mbs: int,\n",
    "                 num_workers: int = 0,\n",
    "                 shuffle: bool = True,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.qa_file_path = qa_file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.gbs = gbs\n",
    "        self.mbs = mbs\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.qa_pairs = None # Data will be loaded in setup\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        # Data loading now happens within the data module's setup\n",
    "        # It's better to pass the full list of Q&A pairs to the DataModule init\n",
    "        # then to load it inside, if the list is already constructed in the main script.\n",
    "        # But for this example, we keep the file path here.\n",
    "        self.qa_pairs = read_qa_pairs_from_file() # Calls the global function\n",
    "        print(f\"[DataModule] Loaded {len(self.qa_pairs)} QA pairs in setup stage.\")\n",
    "        \n",
    "        self.train_dataset = QADataset(\n",
    "            qa_pairs=self.qa_pairs,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        custom_collater = lambda batch: collate_fn(batch, tokenizer=self.tokenizer, max_length=self.max_length)\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.mbs,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=custom_collater,\n",
    "        )\n",
    "\n",
    "# --- 5. Function to configure DataModule CFG ---\n",
    "def a_function_that_configures_your_custom_dataset_cfg(gbs: int, mbs: int, seq_length: int, tokenizer_model_id: str, qa_file_path: str) -> dict:\n",
    "    return {\n",
    "        \"_target_\": f\"{__name__}.CustomQAPairDataModule\",\n",
    "        \"qa_file_path\": qa_file_path,\n",
    "        \"tokenizer\": {\"_target_\": \"transformers.AutoTokenizer.from_pretrained\", \"pretrained_model_name_or_path\": tokenizer_model_id, \"trust_remote_code\": True, \"cache_dir\": os.getenv('HF_HOME')}, # Ensure tokenizer loads to correct cache\n",
    "        \"max_length\": seq_length,\n",
    "        \"gbs\": gbs,\n",
    "        \"mbs\": mbs,\n",
    "        \"num_workers\": 0,\n",
    "        \"shuffle\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    import ast # Need to import ast here as well if not already\n",
    "    \n",
    "    # --- Necessary setup and file definitions ---\n",
    "    cache_dir = '/datasets/soc-20250703225140/'\n",
    "    # os.makedirs(cache_dir, exist_ok=True) # Assuming this is handled by PrimeHub\n",
    "    os.environ['HF_HOME'] = cache_dir\n",
    "    os.environ['HF_HUB_CACHE'] = cache_dir\n",
    "    os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "    os.environ['NEMO_HOME'] = cache_dir\n",
    "    print(f\"Hugging Face and NeMo cache directory set to: {cache_dir}\")\n",
    "\n",
    "    checkpoint_path = \"/datasets/soc-20250703225140/nemo_checkpoints\"\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    # --- Load Tokenizer ---\n",
    "    model_name = \"nvidia/Llama-3_3-Nemotron-Super-49B-v1\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, cache_dir=cache_dir)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(f\"分詞器的 pad_token 已設定為 eos_token: {tokenizer.pad_token_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"錯誤：無法載入分詞器 '{model_name}'。請檢查網路連線或 HFToken。錯誤: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # --- Simulate NeMo Recipe Parameters ---\n",
    "    # These parameters typically come from the NeMo Recipe or its model configuration\n",
    "    num_nodes_actual = 1       # You have 1 node\n",
    "    num_gpus_per_node_actual = 2 # You have 2 A100 80GB GPUs on this node\n",
    "\n",
    "    global_batch_size_recipe = 8   # Global Batch Size\n",
    "    micro_batch_size_per_gpu_recipe = 4 # Micro Batch Size per GPU (adjust based on OOM tests)\n",
    "\n",
    "    # You can specify this directly or derive from a model config if available\n",
    "    model_seq_length_from_recipe = 2048 # Example: 2048 or 4096, adjust based on model/GPU\n",
    "\n",
    "    # --- Instantiate NeMo Recipe ---\n",
    "    recipe = llm.llama33_nemotron_super_49b.finetune_recipe(\n",
    "        name=\"llama33_nemotron_super_49b_finetuning\",\n",
    "        dir=checkpoint_path,\n",
    "        num_nodes=num_nodes_actual,\n",
    "        num_gpus_per_node=num_gpus_per_node_actual,\n",
    "        peft_scheme='lora',\n",
    "        packed_sequence=False,\n",
    "        global_batch_size=global_batch_size_recipe,\n",
    "        micro_batch_size_per_gpu=micro_batch_size_per_gpu_recipe,\n",
    "        seq_length=model_seq_length_from_recipe,\n",
    "    )\n",
    "    print(\"\\nNeMo Recipe 實例已創建。\")\n",
    "\n",
    "    # --- CRITICAL FIX: Set Model Parallelism Parameters ---\n",
    "    # These parameters must be set AFTER the recipe is instantiated\n",
    "    # but BEFORE it is run.\n",
    "    # For 2 GPUs, TP=2, PP=1 is the only valid combination for optimal use.\n",
    "    recipe.trainer.strategy.tensor_model_parallel_size = 2\n",
    "    recipe.trainer.strategy.pipeline_model_parallel_size = 1 # Assuming no pipeline parallelism\n",
    "    # If context parallelism is implicitly used, ensure its size * (TP*PP) divides total GPUs.\n",
    "    # For Llama 3 models, context_parallel_size is often 1.\n",
    "\n",
    "    print(f\"設定模型並行參數: tensor_model_parallel_size={recipe.trainer.strategy.tensor_model_parallel_size}, pipeline_model_parallel_size={recipe.trainer.strategy.pipeline_model_parallel_size}\")\n",
    "\n",
    "\n",
    "    # --- Override the data argument ---\n",
    "    custom_datamodule_cfg = a_function_that_configures_your_custom_dataset_cfg(\n",
    "        gbs=global_batch_size_recipe,\n",
    "        mbs=micro_batch_size_per_gpu_recipe,\n",
    "        seq_length=model_seq_length_from_recipe,\n",
    "        tokenizer_model_id=model_name, # Pass tokenizer_model_id, not the tokenizer instance\n",
    "        qa_file_path=qa_data_file\n",
    "    )\n",
    "    recipe.data = custom_datamodule_cfg\n",
    "    print(\"NeMo Recipe's data argument has been successfully overridden with your custom DataModule configuration.\")\n",
    "\n",
    "    # --- Run the NeMo Recipe ---\n",
    "    print(\"\\n嘗試運行 NeMo Recipe...\")\n",
    "    try:\n",
    "        run.run(recipe, executor=run.LocalExecutor())\n",
    "        print(\"\\nNeMo Recipe 執行成功！\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n運行 NeMo Recipe 時發生錯誤：{e}\")\n",
    "        print(\"請檢查您的 NeMo 配置、數據路徑、以及 GPU 資源是否匹配。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
